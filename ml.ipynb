{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BTC-USD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08-06-2023</td>\n",
       "      <td>26347.65430</td>\n",
       "      <td>26797.51367</td>\n",
       "      <td>26246.66406</td>\n",
       "      <td>26508.21680</td>\n",
       "      <td>26508.21680</td>\n",
       "      <td>1.190482e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09-06-2023</td>\n",
       "      <td>26505.92383</td>\n",
       "      <td>26770.28906</td>\n",
       "      <td>26339.31445</td>\n",
       "      <td>26480.37500</td>\n",
       "      <td>26480.37500</td>\n",
       "      <td>1.101555e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-06-2023</td>\n",
       "      <td>26481.76172</td>\n",
       "      <td>26531.04492</td>\n",
       "      <td>25501.83594</td>\n",
       "      <td>25851.24023</td>\n",
       "      <td>25851.24023</td>\n",
       "      <td>1.987293e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-06-2023</td>\n",
       "      <td>25854.03125</td>\n",
       "      <td>26203.43945</td>\n",
       "      <td>25668.98633</td>\n",
       "      <td>25940.16797</td>\n",
       "      <td>25940.16797</td>\n",
       "      <td>1.073261e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12-06-2023</td>\n",
       "      <td>25934.28516</td>\n",
       "      <td>26087.91992</td>\n",
       "      <td>25675.19727</td>\n",
       "      <td>25902.50000</td>\n",
       "      <td>25902.50000</td>\n",
       "      <td>1.167789e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>04-06-2024</td>\n",
       "      <td>68804.57031</td>\n",
       "      <td>71047.40625</td>\n",
       "      <td>68564.64063</td>\n",
       "      <td>70567.76563</td>\n",
       "      <td>70567.76563</td>\n",
       "      <td>3.314970e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>05-06-2024</td>\n",
       "      <td>70568.35156</td>\n",
       "      <td>71735.41406</td>\n",
       "      <td>70390.71094</td>\n",
       "      <td>71082.82031</td>\n",
       "      <td>71082.82031</td>\n",
       "      <td>3.281077e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>06-06-2024</td>\n",
       "      <td>71082.84375</td>\n",
       "      <td>71625.73438</td>\n",
       "      <td>70119.12500</td>\n",
       "      <td>70757.16406</td>\n",
       "      <td>70757.16406</td>\n",
       "      <td>2.522315e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>07-06-2024</td>\n",
       "      <td>70759.18750</td>\n",
       "      <td>71907.85156</td>\n",
       "      <td>68507.25781</td>\n",
       "      <td>69342.58594</td>\n",
       "      <td>69342.58594</td>\n",
       "      <td>3.618838e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>08-06-2024</td>\n",
       "      <td>69324.17969</td>\n",
       "      <td>69533.32031</td>\n",
       "      <td>69210.74219</td>\n",
       "      <td>69463.69531</td>\n",
       "      <td>69463.69531</td>\n",
       "      <td>2.692020e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date         Open         High          Low        Close  \\\n",
       "0    08-06-2023  26347.65430  26797.51367  26246.66406  26508.21680   \n",
       "1    09-06-2023  26505.92383  26770.28906  26339.31445  26480.37500   \n",
       "2    10-06-2023  26481.76172  26531.04492  25501.83594  25851.24023   \n",
       "3    11-06-2023  25854.03125  26203.43945  25668.98633  25940.16797   \n",
       "4    12-06-2023  25934.28516  26087.91992  25675.19727  25902.50000   \n",
       "..          ...          ...          ...          ...          ...   \n",
       "362  04-06-2024  68804.57031  71047.40625  68564.64063  70567.76563   \n",
       "363  05-06-2024  70568.35156  71735.41406  70390.71094  71082.82031   \n",
       "364  06-06-2024  71082.84375  71625.73438  70119.12500  70757.16406   \n",
       "365  07-06-2024  70759.18750  71907.85156  68507.25781  69342.58594   \n",
       "366  08-06-2024  69324.17969  69533.32031  69210.74219  69463.69531   \n",
       "\n",
       "       Adj Close        Volume  \n",
       "0    26508.21680  1.190482e+10  \n",
       "1    26480.37500  1.101555e+10  \n",
       "2    25851.24023  1.987293e+10  \n",
       "3    25940.16797  1.073261e+10  \n",
       "4    25902.50000  1.167789e+10  \n",
       "..           ...           ...  \n",
       "362  70567.76563  3.314970e+10  \n",
       "363  71082.82031  3.281077e+10  \n",
       "364  70757.16406  2.522315e+10  \n",
       "365  69342.58594  3.618838e+10  \n",
       "366  69463.69531  2.692020e+10  \n",
       "\n",
       "[367 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 367 entries, 0 to 366\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       367 non-null    object \n",
      " 1   Open       367 non-null    float64\n",
      " 2   High       367 non-null    float64\n",
      " 3   Low        367 non-null    float64\n",
      " 4   Close      367 non-null    float64\n",
      " 5   Adj Close  367 non-null    float64\n",
      " 6   Volume     367 non-null    float64\n",
      "dtypes: float64(6), object(1)\n",
      "memory usage: 20.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1            NaN\n",
       "2            NaN\n",
       "3            NaN\n",
       "4            NaN\n",
       "5            NaN\n",
       "         ...    \n",
       "362    51.839588\n",
       "363    58.735150\n",
       "364    63.653717\n",
       "365    53.653915\n",
       "366    50.936849\n",
       "Name: Close, Length: 366, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Engineering\n",
    "\n",
    "#Finding the Relative Strength Index\n",
    "\n",
    "delta = df['Close'].diff(1)\n",
    "delta.dropna(inplace = True)\n",
    "positive = delta.copy()\n",
    "negative = delta.copy()\n",
    "\n",
    "positive[ positive < 0] = 0\n",
    "negative[ negative > 0] = 0\n",
    "\n",
    "days = 14\n",
    "\n",
    "average_gain = positive.rolling(window = days).mean()\n",
    "average_loss = abs(negative.rolling(window = days).mean())\n",
    "\n",
    "relative_strength = average_gain/average_loss\n",
    "\n",
    "RSI = 100.0 - (100.0/ (1.0 + relative_strength))\n",
    "\n",
    "RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>RSI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08-06-2023</td>\n",
       "      <td>26347.65430</td>\n",
       "      <td>26797.51367</td>\n",
       "      <td>26246.66406</td>\n",
       "      <td>26508.21680</td>\n",
       "      <td>26508.21680</td>\n",
       "      <td>1.190482e+10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09-06-2023</td>\n",
       "      <td>26505.92383</td>\n",
       "      <td>26770.28906</td>\n",
       "      <td>26339.31445</td>\n",
       "      <td>26480.37500</td>\n",
       "      <td>26480.37500</td>\n",
       "      <td>1.101555e+10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-06-2023</td>\n",
       "      <td>26481.76172</td>\n",
       "      <td>26531.04492</td>\n",
       "      <td>25501.83594</td>\n",
       "      <td>25851.24023</td>\n",
       "      <td>25851.24023</td>\n",
       "      <td>1.987293e+10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-06-2023</td>\n",
       "      <td>25854.03125</td>\n",
       "      <td>26203.43945</td>\n",
       "      <td>25668.98633</td>\n",
       "      <td>25940.16797</td>\n",
       "      <td>25940.16797</td>\n",
       "      <td>1.073261e+10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12-06-2023</td>\n",
       "      <td>25934.28516</td>\n",
       "      <td>26087.91992</td>\n",
       "      <td>25675.19727</td>\n",
       "      <td>25902.50000</td>\n",
       "      <td>25902.50000</td>\n",
       "      <td>1.167789e+10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>04-06-2024</td>\n",
       "      <td>68804.57031</td>\n",
       "      <td>71047.40625</td>\n",
       "      <td>68564.64063</td>\n",
       "      <td>70567.76563</td>\n",
       "      <td>70567.76563</td>\n",
       "      <td>3.314970e+10</td>\n",
       "      <td>51.839588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>05-06-2024</td>\n",
       "      <td>70568.35156</td>\n",
       "      <td>71735.41406</td>\n",
       "      <td>70390.71094</td>\n",
       "      <td>71082.82031</td>\n",
       "      <td>71082.82031</td>\n",
       "      <td>3.281077e+10</td>\n",
       "      <td>58.735150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>06-06-2024</td>\n",
       "      <td>71082.84375</td>\n",
       "      <td>71625.73438</td>\n",
       "      <td>70119.12500</td>\n",
       "      <td>70757.16406</td>\n",
       "      <td>70757.16406</td>\n",
       "      <td>2.522315e+10</td>\n",
       "      <td>63.653717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>07-06-2024</td>\n",
       "      <td>70759.18750</td>\n",
       "      <td>71907.85156</td>\n",
       "      <td>68507.25781</td>\n",
       "      <td>69342.58594</td>\n",
       "      <td>69342.58594</td>\n",
       "      <td>3.618838e+10</td>\n",
       "      <td>53.653915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>08-06-2024</td>\n",
       "      <td>69324.17969</td>\n",
       "      <td>69533.32031</td>\n",
       "      <td>69210.74219</td>\n",
       "      <td>69463.69531</td>\n",
       "      <td>69463.69531</td>\n",
       "      <td>2.692020e+10</td>\n",
       "      <td>50.936849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date         Open         High          Low        Close  \\\n",
       "0    08-06-2023  26347.65430  26797.51367  26246.66406  26508.21680   \n",
       "1    09-06-2023  26505.92383  26770.28906  26339.31445  26480.37500   \n",
       "2    10-06-2023  26481.76172  26531.04492  25501.83594  25851.24023   \n",
       "3    11-06-2023  25854.03125  26203.43945  25668.98633  25940.16797   \n",
       "4    12-06-2023  25934.28516  26087.91992  25675.19727  25902.50000   \n",
       "..          ...          ...          ...          ...          ...   \n",
       "362  04-06-2024  68804.57031  71047.40625  68564.64063  70567.76563   \n",
       "363  05-06-2024  70568.35156  71735.41406  70390.71094  71082.82031   \n",
       "364  06-06-2024  71082.84375  71625.73438  70119.12500  70757.16406   \n",
       "365  07-06-2024  70759.18750  71907.85156  68507.25781  69342.58594   \n",
       "366  08-06-2024  69324.17969  69533.32031  69210.74219  69463.69531   \n",
       "\n",
       "       Adj Close        Volume        RSI  \n",
       "0    26508.21680  1.190482e+10   0.000000  \n",
       "1    26480.37500  1.101555e+10   0.000000  \n",
       "2    25851.24023  1.987293e+10   0.000000  \n",
       "3    25940.16797  1.073261e+10   0.000000  \n",
       "4    25902.50000  1.167789e+10   0.000000  \n",
       "..           ...           ...        ...  \n",
       "362  70567.76563  3.314970e+10  51.839588  \n",
       "363  71082.82031  3.281077e+10  58.735150  \n",
       "364  70757.16406  2.522315e+10  63.653717  \n",
       "365  69342.58594  3.618838e+10  53.653915  \n",
       "366  69463.69531  2.692020e+10  50.936849  \n",
       "\n",
       "[367 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['RSI'] = RSI\n",
    "df['RSI'] = df['RSI'].fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               NaN\n",
       "1               NaN\n",
       "2               NaN\n",
       "3               NaN\n",
       "4      26136.500000\n",
       "           ...     \n",
       "362    68464.500000\n",
       "363    69182.781250\n",
       "364    69792.826562\n",
       "365    70111.023438\n",
       "366    70242.806250\n",
       "Name: Close, Length: 367, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding Simple Moving Average\n",
    "\n",
    "period = 5\n",
    "SMA = df['Close'].rolling(window= period).mean()\n",
    "\n",
    "SMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>RSI</th>\n",
       "      <th>SMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08-06-2023</td>\n",
       "      <td>26347.65430</td>\n",
       "      <td>26797.51367</td>\n",
       "      <td>26246.66406</td>\n",
       "      <td>26508.21680</td>\n",
       "      <td>26508.21680</td>\n",
       "      <td>1.190482e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09-06-2023</td>\n",
       "      <td>26505.92383</td>\n",
       "      <td>26770.28906</td>\n",
       "      <td>26339.31445</td>\n",
       "      <td>26480.37500</td>\n",
       "      <td>26480.37500</td>\n",
       "      <td>1.101555e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-06-2023</td>\n",
       "      <td>26481.76172</td>\n",
       "      <td>26531.04492</td>\n",
       "      <td>25501.83594</td>\n",
       "      <td>25851.24023</td>\n",
       "      <td>25851.24023</td>\n",
       "      <td>1.987293e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-06-2023</td>\n",
       "      <td>25854.03125</td>\n",
       "      <td>26203.43945</td>\n",
       "      <td>25668.98633</td>\n",
       "      <td>25940.16797</td>\n",
       "      <td>25940.16797</td>\n",
       "      <td>1.073261e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12-06-2023</td>\n",
       "      <td>25934.28516</td>\n",
       "      <td>26087.91992</td>\n",
       "      <td>25675.19727</td>\n",
       "      <td>25902.50000</td>\n",
       "      <td>25902.50000</td>\n",
       "      <td>1.167789e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26136.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>04-06-2024</td>\n",
       "      <td>68804.57031</td>\n",
       "      <td>71047.40625</td>\n",
       "      <td>68564.64063</td>\n",
       "      <td>70567.76563</td>\n",
       "      <td>70567.76563</td>\n",
       "      <td>3.314970e+10</td>\n",
       "      <td>51.839588</td>\n",
       "      <td>68464.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>05-06-2024</td>\n",
       "      <td>70568.35156</td>\n",
       "      <td>71735.41406</td>\n",
       "      <td>70390.71094</td>\n",
       "      <td>71082.82031</td>\n",
       "      <td>71082.82031</td>\n",
       "      <td>3.281077e+10</td>\n",
       "      <td>58.735150</td>\n",
       "      <td>69182.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>06-06-2024</td>\n",
       "      <td>71082.84375</td>\n",
       "      <td>71625.73438</td>\n",
       "      <td>70119.12500</td>\n",
       "      <td>70757.16406</td>\n",
       "      <td>70757.16406</td>\n",
       "      <td>2.522315e+10</td>\n",
       "      <td>63.653717</td>\n",
       "      <td>69792.826562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>07-06-2024</td>\n",
       "      <td>70759.18750</td>\n",
       "      <td>71907.85156</td>\n",
       "      <td>68507.25781</td>\n",
       "      <td>69342.58594</td>\n",
       "      <td>69342.58594</td>\n",
       "      <td>3.618838e+10</td>\n",
       "      <td>53.653915</td>\n",
       "      <td>70111.023438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>08-06-2024</td>\n",
       "      <td>69324.17969</td>\n",
       "      <td>69533.32031</td>\n",
       "      <td>69210.74219</td>\n",
       "      <td>69463.69531</td>\n",
       "      <td>69463.69531</td>\n",
       "      <td>2.692020e+10</td>\n",
       "      <td>50.936849</td>\n",
       "      <td>70242.806250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date         Open         High          Low        Close  \\\n",
       "0    08-06-2023  26347.65430  26797.51367  26246.66406  26508.21680   \n",
       "1    09-06-2023  26505.92383  26770.28906  26339.31445  26480.37500   \n",
       "2    10-06-2023  26481.76172  26531.04492  25501.83594  25851.24023   \n",
       "3    11-06-2023  25854.03125  26203.43945  25668.98633  25940.16797   \n",
       "4    12-06-2023  25934.28516  26087.91992  25675.19727  25902.50000   \n",
       "..          ...          ...          ...          ...          ...   \n",
       "362  04-06-2024  68804.57031  71047.40625  68564.64063  70567.76563   \n",
       "363  05-06-2024  70568.35156  71735.41406  70390.71094  71082.82031   \n",
       "364  06-06-2024  71082.84375  71625.73438  70119.12500  70757.16406   \n",
       "365  07-06-2024  70759.18750  71907.85156  68507.25781  69342.58594   \n",
       "366  08-06-2024  69324.17969  69533.32031  69210.74219  69463.69531   \n",
       "\n",
       "       Adj Close        Volume        RSI           SMA  \n",
       "0    26508.21680  1.190482e+10   0.000000      0.000000  \n",
       "1    26480.37500  1.101555e+10   0.000000      0.000000  \n",
       "2    25851.24023  1.987293e+10   0.000000      0.000000  \n",
       "3    25940.16797  1.073261e+10   0.000000      0.000000  \n",
       "4    25902.50000  1.167789e+10   0.000000  26136.500000  \n",
       "..           ...           ...        ...           ...  \n",
       "362  70567.76563  3.314970e+10  51.839588  68464.500000  \n",
       "363  71082.82031  3.281077e+10  58.735150  69182.781250  \n",
       "364  70757.16406  2.522315e+10  63.653717  69792.826562  \n",
       "365  69342.58594  3.618838e+10  53.653915  70111.023438  \n",
       "366  69463.69531  2.692020e+10  50.936849  70242.806250  \n",
       "\n",
       "[367 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SMA'] = SMA\n",
    "df['SMA'] = df['SMA'].fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      26508.216800\n",
       "1      26498.936200\n",
       "2      26283.037543\n",
       "3      26168.747686\n",
       "4      26079.998457\n",
       "           ...     \n",
       "362    68980.494101\n",
       "363    69681.269504\n",
       "364    70039.901023\n",
       "365    69807.462662\n",
       "366    69692.873545\n",
       "Name: Close, Length: 367, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding Exponential Moving Average\n",
    "\n",
    "EMA = df['Close'].ewm(span = 5, min_periods = 0, adjust = False).mean()\n",
    "EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>RSI</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08-06-2023</td>\n",
       "      <td>26347.65430</td>\n",
       "      <td>26797.51367</td>\n",
       "      <td>26246.66406</td>\n",
       "      <td>26508.21680</td>\n",
       "      <td>26508.21680</td>\n",
       "      <td>1.190482e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26508.216800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09-06-2023</td>\n",
       "      <td>26505.92383</td>\n",
       "      <td>26770.28906</td>\n",
       "      <td>26339.31445</td>\n",
       "      <td>26480.37500</td>\n",
       "      <td>26480.37500</td>\n",
       "      <td>1.101555e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26498.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-06-2023</td>\n",
       "      <td>26481.76172</td>\n",
       "      <td>26531.04492</td>\n",
       "      <td>25501.83594</td>\n",
       "      <td>25851.24023</td>\n",
       "      <td>25851.24023</td>\n",
       "      <td>1.987293e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26283.037543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-06-2023</td>\n",
       "      <td>25854.03125</td>\n",
       "      <td>26203.43945</td>\n",
       "      <td>25668.98633</td>\n",
       "      <td>25940.16797</td>\n",
       "      <td>25940.16797</td>\n",
       "      <td>1.073261e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26168.747686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12-06-2023</td>\n",
       "      <td>25934.28516</td>\n",
       "      <td>26087.91992</td>\n",
       "      <td>25675.19727</td>\n",
       "      <td>25902.50000</td>\n",
       "      <td>25902.50000</td>\n",
       "      <td>1.167789e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26136.500000</td>\n",
       "      <td>26079.998457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>04-06-2024</td>\n",
       "      <td>68804.57031</td>\n",
       "      <td>71047.40625</td>\n",
       "      <td>68564.64063</td>\n",
       "      <td>70567.76563</td>\n",
       "      <td>70567.76563</td>\n",
       "      <td>3.314970e+10</td>\n",
       "      <td>51.839588</td>\n",
       "      <td>68464.500000</td>\n",
       "      <td>68980.494101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>05-06-2024</td>\n",
       "      <td>70568.35156</td>\n",
       "      <td>71735.41406</td>\n",
       "      <td>70390.71094</td>\n",
       "      <td>71082.82031</td>\n",
       "      <td>71082.82031</td>\n",
       "      <td>3.281077e+10</td>\n",
       "      <td>58.735150</td>\n",
       "      <td>69182.781250</td>\n",
       "      <td>69681.269504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>06-06-2024</td>\n",
       "      <td>71082.84375</td>\n",
       "      <td>71625.73438</td>\n",
       "      <td>70119.12500</td>\n",
       "      <td>70757.16406</td>\n",
       "      <td>70757.16406</td>\n",
       "      <td>2.522315e+10</td>\n",
       "      <td>63.653717</td>\n",
       "      <td>69792.826562</td>\n",
       "      <td>70039.901023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>07-06-2024</td>\n",
       "      <td>70759.18750</td>\n",
       "      <td>71907.85156</td>\n",
       "      <td>68507.25781</td>\n",
       "      <td>69342.58594</td>\n",
       "      <td>69342.58594</td>\n",
       "      <td>3.618838e+10</td>\n",
       "      <td>53.653915</td>\n",
       "      <td>70111.023438</td>\n",
       "      <td>69807.462662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>08-06-2024</td>\n",
       "      <td>69324.17969</td>\n",
       "      <td>69533.32031</td>\n",
       "      <td>69210.74219</td>\n",
       "      <td>69463.69531</td>\n",
       "      <td>69463.69531</td>\n",
       "      <td>2.692020e+10</td>\n",
       "      <td>50.936849</td>\n",
       "      <td>70242.806250</td>\n",
       "      <td>69692.873545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date         Open         High          Low        Close  \\\n",
       "0    08-06-2023  26347.65430  26797.51367  26246.66406  26508.21680   \n",
       "1    09-06-2023  26505.92383  26770.28906  26339.31445  26480.37500   \n",
       "2    10-06-2023  26481.76172  26531.04492  25501.83594  25851.24023   \n",
       "3    11-06-2023  25854.03125  26203.43945  25668.98633  25940.16797   \n",
       "4    12-06-2023  25934.28516  26087.91992  25675.19727  25902.50000   \n",
       "..          ...          ...          ...          ...          ...   \n",
       "362  04-06-2024  68804.57031  71047.40625  68564.64063  70567.76563   \n",
       "363  05-06-2024  70568.35156  71735.41406  70390.71094  71082.82031   \n",
       "364  06-06-2024  71082.84375  71625.73438  70119.12500  70757.16406   \n",
       "365  07-06-2024  70759.18750  71907.85156  68507.25781  69342.58594   \n",
       "366  08-06-2024  69324.17969  69533.32031  69210.74219  69463.69531   \n",
       "\n",
       "       Adj Close        Volume        RSI           SMA           EMA  \n",
       "0    26508.21680  1.190482e+10   0.000000      0.000000  26508.216800  \n",
       "1    26480.37500  1.101555e+10   0.000000      0.000000  26498.936200  \n",
       "2    25851.24023  1.987293e+10   0.000000      0.000000  26283.037543  \n",
       "3    25940.16797  1.073261e+10   0.000000      0.000000  26168.747686  \n",
       "4    25902.50000  1.167789e+10   0.000000  26136.500000  26079.998457  \n",
       "..           ...           ...        ...           ...           ...  \n",
       "362  70567.76563  3.314970e+10  51.839588  68464.500000  68980.494101  \n",
       "363  71082.82031  3.281077e+10  58.735150  69182.781250  69681.269504  \n",
       "364  70757.16406  2.522315e+10  63.653717  69792.826562  70039.901023  \n",
       "365  69342.58594  3.618838e+10  53.653915  70111.023438  69807.462662  \n",
       "366  69463.69531  2.692020e+10  50.936849  70242.806250  69692.873545  \n",
       "\n",
       "[367 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['EMA'] = EMA\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>RSI</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08-06-2023</td>\n",
       "      <td>26508.21680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26508.216800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09-06-2023</td>\n",
       "      <td>26480.37500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26498.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-06-2023</td>\n",
       "      <td>25851.24023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26283.037543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-06-2023</td>\n",
       "      <td>25940.16797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26168.747686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12-06-2023</td>\n",
       "      <td>25902.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26136.500000</td>\n",
       "      <td>26079.998457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>04-06-2024</td>\n",
       "      <td>70567.76563</td>\n",
       "      <td>51.839588</td>\n",
       "      <td>68464.500000</td>\n",
       "      <td>68980.494101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>05-06-2024</td>\n",
       "      <td>71082.82031</td>\n",
       "      <td>58.735150</td>\n",
       "      <td>69182.781250</td>\n",
       "      <td>69681.269504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>06-06-2024</td>\n",
       "      <td>70757.16406</td>\n",
       "      <td>63.653717</td>\n",
       "      <td>69792.826562</td>\n",
       "      <td>70039.901023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>07-06-2024</td>\n",
       "      <td>69342.58594</td>\n",
       "      <td>53.653915</td>\n",
       "      <td>70111.023438</td>\n",
       "      <td>69807.462662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>08-06-2024</td>\n",
       "      <td>69463.69531</td>\n",
       "      <td>50.936849</td>\n",
       "      <td>70242.806250</td>\n",
       "      <td>69692.873545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Close        RSI           SMA           EMA\n",
       "0    08-06-2023  26508.21680   0.000000      0.000000  26508.216800\n",
       "1    09-06-2023  26480.37500   0.000000      0.000000  26498.936200\n",
       "2    10-06-2023  25851.24023   0.000000      0.000000  26283.037543\n",
       "3    11-06-2023  25940.16797   0.000000      0.000000  26168.747686\n",
       "4    12-06-2023  25902.50000   0.000000  26136.500000  26079.998457\n",
       "..          ...          ...        ...           ...           ...\n",
       "362  04-06-2024  70567.76563  51.839588  68464.500000  68980.494101\n",
       "363  05-06-2024  71082.82031  58.735150  69182.781250  69681.269504\n",
       "364  06-06-2024  70757.16406  63.653717  69792.826562  70039.901023\n",
       "365  07-06-2024  69342.58594  53.653915  70111.023438  69807.462662\n",
       "366  08-06-2024  69463.69531  50.936849  70242.806250  69692.873545\n",
       "\n",
       "[367 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['Date','Close','RSI', 'SMA', 'EMA']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           RSI           SMA           EMA\n",
      "63   54.439288  29395.802346  29422.500533\n",
      "241  56.338913  42884.014064  42785.805284\n",
      "306  47.931312  69373.335942  69457.482293\n",
      "317  41.975580  63487.864064  64160.680754\n",
      "245  82.920868  43589.342190  43996.649282\n",
      "..         ...           ...           ...\n",
      "323  41.273648  65151.775782  64545.888768\n",
      "192  55.392152  42289.850782  42029.856543\n",
      "117  53.005535  27364.830080  27343.445873\n",
      "47   34.973361  29633.878516  29542.077807\n",
      "172  53.463998  37507.996874  37387.434614\n",
      "\n",
      "[293 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Splitting the dataset into training and testing dataset\n",
    "\n",
    "X = df[['RSI', \"SMA\", \"EMA\"]]\n",
    "y = df['Close']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.44392884e+01, 2.93958023e+04, 2.94225005e+04],\n",
       "       [5.63389126e+01, 4.28840141e+04, 4.27858053e+04],\n",
       "       [4.79313117e+01, 6.93733359e+04, 6.94574823e+04],\n",
       "       [4.19755800e+01, 6.34878641e+04, 6.41606808e+04],\n",
       "       [8.29208680e+01, 4.35893422e+04, 4.39966493e+04],\n",
       "       [3.51501522e+01, 6.09937305e+04, 6.05476952e+04],\n",
       "       [8.24578708e+01, 3.03942309e+04, 3.02941580e+04],\n",
       "       [8.92402396e+01, 3.42393734e+04, 3.41240964e+04],\n",
       "       [9.24809039e+01, 3.46470906e+04, 3.46826344e+04],\n",
       "       [1.84940109e+01, 2.94109016e+04, 2.93900361e+04],\n",
       "       [5.87351503e+01, 6.91827812e+04, 6.96812695e+04],\n",
       "       [5.24648784e+01, 6.14978242e+04, 6.18356551e+04],\n",
       "       [4.40549906e+01, 6.97229469e+04, 6.90051056e+04],\n",
       "       [5.08964018e+01, 4.28607312e+04, 4.27213467e+04],\n",
       "       [8.02235849e+01, 6.22682047e+04, 6.13675131e+04],\n",
       "       [6.36597371e+01, 7.14978125e+04, 7.05519088e+04],\n",
       "       [4.33999281e+01, 6.46317602e+04, 6.50071789e+04],\n",
       "       [8.05974877e+01, 3.03936289e+04, 3.03551893e+04],\n",
       "       [6.41096822e+01, 4.31293586e+04, 4.36648907e+04],\n",
       "       [2.25649291e+01, 4.08365391e+04, 4.06579622e+04],\n",
       "       [6.31947300e+01, 4.36471711e+04, 4.34353960e+04],\n",
       "       [7.14504849e+01, 2.93286813e+04, 2.94109010e+04],\n",
       "       [5.97135840e+01, 2.72831438e+04, 2.73001795e+04],\n",
       "       [5.01570825e+01, 2.94175359e+04, 2.93708474e+04],\n",
       "       [6.91058990e+01, 2.87332625e+04, 2.87198988e+04],\n",
       "       [0.00000000e+00, 2.68705738e+04, 2.70440809e+04],\n",
       "       [6.52118718e+01, 6.87268516e+04, 6.89100643e+04],\n",
       "       [6.95207783e+01, 3.69404039e+04, 3.66537115e+04],\n",
       "       [5.75124717e+01, 6.99452531e+04, 6.95387567e+04],\n",
       "       [6.18596240e+01, 3.76271227e+04, 3.76662774e+04],\n",
       "       [8.14720141e+01, 3.03328008e+04, 2.99889288e+04],\n",
       "       [6.60260231e+01, 4.60099391e+04, 4.59642118e+04],\n",
       "       [9.06762282e+01, 3.43416000e+04, 3.43053247e+04],\n",
       "       [4.07247301e+01, 6.37108203e+04, 6.44160020e+04],\n",
       "       [5.13602622e+01, 4.11966805e+04, 4.15040033e+04],\n",
       "       [5.25712160e+01, 4.57919531e+04, 4.49271972e+04],\n",
       "       [6.88867811e+01, 2.67353121e+04, 2.67604230e+04],\n",
       "       [5.38332156e+01, 6.86105813e+04, 6.83296640e+04],\n",
       "       [5.00661564e+01, 4.35058750e+04, 4.35117270e+04],\n",
       "       [4.85404363e+01, 6.84109406e+04, 6.82946975e+04],\n",
       "       [7.94615096e+01, 6.10518047e+04, 6.04675841e+04],\n",
       "       [7.37216917e+01, 3.95672633e+04, 3.99871616e+04],\n",
       "       [4.54786056e+01, 6.84217773e+04, 6.76449694e+04],\n",
       "       [8.24514237e+01, 3.50478742e+04, 3.49262727e+04],\n",
       "       [4.02182294e+01, 4.22580062e+04, 4.21536467e+04],\n",
       "       [1.39305856e+01, 2.60828695e+04, 2.61243720e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 2.64989362e+04],\n",
       "       [6.47364041e+01, 3.52535523e+04, 3.53006866e+04],\n",
       "       [0.00000000e+00, 2.57699523e+04, 2.58931174e+04],\n",
       "       [7.77910828e+01, 5.74088367e+04, 5.83093621e+04],\n",
       "       [8.34947590e+01, 4.55245875e+04, 4.59549808e+04],\n",
       "       [8.42064854e+01, 3.60286609e+04, 3.62812114e+04],\n",
       "       [4.68878757e+01, 2.73079812e+04, 2.71656016e+04],\n",
       "       [5.83965198e+01, 6.95179219e+04, 6.86677494e+04],\n",
       "       [6.58564358e+01, 2.82286031e+04, 2.82383736e+04],\n",
       "       [7.46206644e+01, 6.46871273e+04, 6.45196370e+04],\n",
       "       [9.34975641e+01, 5.19421992e+04, 5.15051959e+04],\n",
       "       [4.91541055e+01, 3.03682926e+04, 3.04300628e+04],\n",
       "       [5.83582898e+01, 6.54089086e+04, 6.56373442e+04],\n",
       "       [5.42413476e+01, 3.03781688e+04, 3.04172574e+04],\n",
       "       [5.09368486e+01, 7.02428062e+04, 6.96928735e+04],\n",
       "       [3.81541537e+01, 2.57904453e+04, 2.58868430e+04],\n",
       "       [4.73899002e+01, 2.65726012e+04, 2.63057081e+04],\n",
       "       [6.42931982e+01, 4.50410438e+04, 4.53291484e+04],\n",
       "       [3.87176291e+01, 6.81609695e+04, 6.72772279e+04],\n",
       "       [6.04057782e+01, 3.65344062e+04, 3.65953556e+04],\n",
       "       [5.13822329e+01, 6.82250547e+04, 6.80580981e+04],\n",
       "       [6.68909076e+01, 4.32417562e+04, 4.27179757e+04],\n",
       "       [5.26547597e+01, 3.70798555e+04, 3.70357327e+04],\n",
       "       [5.55211513e+01, 2.66230852e+04, 2.65309602e+04],\n",
       "       [3.70899047e+01, 2.97468035e+04, 2.96994214e+04],\n",
       "       [5.57497685e+01, 6.81171922e+04, 6.86093937e+04],\n",
       "       [8.65201322e+01, 4.18523828e+04, 4.21499199e+04],\n",
       "       [2.06988003e+01, 2.67403352e+04, 2.67716416e+04],\n",
       "       [8.64895845e+01, 3.02802598e+04, 3.06360121e+04],\n",
       "       [6.65646312e+01, 6.83711547e+04, 6.85233398e+04],\n",
       "       [5.73253175e+01, 3.69390719e+04, 3.69087885e+04],\n",
       "       [5.56320774e+01, 7.01360547e+04, 6.88063116e+04],\n",
       "       [4.57886952e+01, 3.05399582e+04, 3.03883620e+04],\n",
       "       [7.10669308e+01, 5.15464156e+04, 5.14220173e+04],\n",
       "       [8.52615014e+01, 4.65664367e+04, 4.67346266e+04],\n",
       "       [5.38153427e+01, 7.02637766e+04, 6.99920854e+04],\n",
       "       [4.36450005e+01, 6.54019219e+04, 6.56179097e+04],\n",
       "       [5.60048602e+01, 3.07318070e+04, 3.06624464e+04],\n",
       "       [2.10108541e+01, 4.01819172e+04, 4.02874914e+04],\n",
       "       [6.47276843e+01, 3.78690484e+04, 3.80071016e+04],\n",
       "       [8.06561044e+01, 3.64487969e+04, 3.65668245e+04],\n",
       "       [5.18991008e+01, 3.75436312e+04, 3.74540679e+04],\n",
       "       [8.98804274e+01, 3.13166039e+04, 3.17245172e+04],\n",
       "       [6.25054562e+01, 4.21077055e+04, 4.24236206e+04],\n",
       "       [7.90243794e+01, 6.77702109e+04, 6.77016468e+04],\n",
       "       [8.28420929e+01, 4.45020656e+04, 4.50468326e+04],\n",
       "       [9.22081483e+01, 5.03519664e+04, 5.03648222e+04],\n",
       "       [6.19139990e+01, 3.06148703e+04, 3.07701879e+04],\n",
       "       [2.07936625e+01, 4.05282727e+04, 4.04643329e+04],\n",
       "       [6.30245507e+01, 4.55340609e+04, 4.57620247e+04],\n",
       "       [4.62990331e+01, 4.07586719e+04, 4.12382080e+04],\n",
       "       [4.88792242e+01, 4.24426750e+04, 4.23833704e+04],\n",
       "       [4.33868729e+01, 2.98486098e+04, 2.98987416e+04],\n",
       "       [5.92999861e+01, 6.84303906e+04, 6.83414401e+04],\n",
       "       [3.81453047e+01, 2.69027707e+04, 2.70511639e+04],\n",
       "       [4.88401949e+01, 2.59149492e+04, 2.59412301e+04],\n",
       "       [5.26261794e+01, 2.63409328e+04, 2.63673909e+04],\n",
       "       [5.17264644e+01, 3.67719406e+04, 3.66470129e+04],\n",
       "       [8.70383147e+01, 4.85826117e+04, 4.84535862e+04],\n",
       "       [3.84816225e+01, 4.22935789e+04, 4.24212669e+04],\n",
       "       [7.95802577e+01, 3.03815676e+04, 3.00829962e+04],\n",
       "       [8.57285743e+01, 5.19377484e+04, 5.18303708e+04],\n",
       "       [1.40451456e+01, 2.61363602e+04, 2.61553775e+04],\n",
       "       [3.72347444e+01, 6.53860406e+04, 6.49411730e+04],\n",
       "       [1.82254485e+01, 2.79989355e+04, 2.75459030e+04],\n",
       "       [1.88090800e+01, 4.11198406e+04, 4.10641679e+04],\n",
       "       [5.26421337e+01, 6.99233844e+04, 6.98342825e+04],\n",
       "       [6.94088880e+01, 6.89960953e+04, 6.87426818e+04],\n",
       "       [5.25794083e+01, 4.27560266e+04, 4.30977773e+04],\n",
       "       [6.62986503e+01, 3.68226297e+04, 3.68146526e+04],\n",
       "       [3.82719601e+01, 6.80239453e+04, 6.81868583e+04],\n",
       "       [5.69736670e+01, 3.66259734e+04, 3.65946916e+04],\n",
       "       [3.81423536e+01, 2.66457930e+04, 2.68715633e+04],\n",
       "       [7.06820822e+01, 4.24776938e+04, 4.26705520e+04],\n",
       "       [6.36537169e+01, 6.97928266e+04, 7.00399010e+04],\n",
       "       [5.84881663e+01, 2.75423648e+04, 2.74954288e+04],\n",
       "       [4.69472826e+01, 6.09591219e+04, 6.21827634e+04],\n",
       "       [4.91092982e+01, 4.25958219e+04, 4.24497821e+04],\n",
       "       [6.91604165e+01, 6.25927672e+04, 6.33129341e+04],\n",
       "       [5.73046192e+01, 2.63863031e+04, 2.63747280e+04],\n",
       "       [8.29662830e+01, 3.49715594e+04, 3.48647314e+04],\n",
       "       [7.73085956e+01, 3.67709883e+04, 3.67293895e+04],\n",
       "       [6.79314069e+01, 3.50689617e+04, 3.51233912e+04],\n",
       "       [8.24705865e+01, 2.99022461e+04, 2.97432624e+04],\n",
       "       [6.13401309e+01, 4.27869656e+04, 4.26584459e+04],\n",
       "       [6.59793073e+00, 2.60982285e+04, 2.63810907e+04],\n",
       "       [6.76519028e+01, 2.64563797e+04, 2.64534669e+04],\n",
       "       [5.53640540e+01, 7.02144156e+04, 7.01370540e+04],\n",
       "       [4.95482201e+01, 4.25694836e+04, 4.25331986e+04],\n",
       "       [9.22900748e+01, 5.11252234e+04, 5.09632825e+04],\n",
       "       [9.62611008e+01, 3.22805781e+04, 3.26506182e+04],\n",
       "       [5.47874933e+01, 6.77786078e+04, 6.78778969e+04],\n",
       "       [0.00000000e+00, 2.61365000e+04, 2.60799985e+04],\n",
       "       [2.91726784e+01, 2.57259113e+04, 2.57161025e+04],\n",
       "       [7.44530243e+01, 2.82908617e+04, 2.86630846e+04],\n",
       "       [6.12035563e+01, 2.92208289e+04, 2.93476853e+04],\n",
       "       [5.35649528e+01, 2.93869258e+04, 2.93833794e+04],\n",
       "       [4.17541987e+01, 6.31742180e+04, 6.37438004e+04],\n",
       "       [4.12674141e+01, 4.20857211e+04, 4.22420096e+04],\n",
       "       [7.75589300e+01, 3.55757383e+04, 3.57648327e+04],\n",
       "       [4.95784887e+01, 4.26096437e+04, 4.30008710e+04],\n",
       "       [7.06130587e+01, 7.14210156e+04, 7.11259765e+04],\n",
       "       [6.59858232e+01, 6.88001828e+04, 6.87054491e+04],\n",
       "       [4.77008607e+01, 6.17972586e+04, 6.19770881e+04],\n",
       "       [4.05149533e+01, 3.01762918e+04, 3.01741514e+04],\n",
       "       [5.88391161e+01, 2.64293641e+04, 2.65854429e+04],\n",
       "       [0.00000000e+00, 2.76105406e+04, 2.80384862e+04],\n",
       "       [5.95328312e+01, 2.68139434e+04, 2.66680272e+04],\n",
       "       [4.09790222e+01, 4.20887297e+04, 4.19909598e+04],\n",
       "       [7.55482643e+01, 6.98847922e+04, 6.99442519e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 2.61687477e+04],\n",
       "       [8.59148785e+01, 4.76944367e+04, 4.78091586e+04],\n",
       "       [5.54164053e+01, 2.75503894e+04, 2.73700030e+04],\n",
       "       [8.26213769e+01, 3.04439395e+04, 3.04959136e+04],\n",
       "       [4.68734699e+01, 3.03671785e+04, 3.03564217e+04],\n",
       "       [1.47534069e+01, 2.61878789e+04, 2.63194181e+04],\n",
       "       [4.45263556e+01, 4.02361344e+04, 4.07972847e+04],\n",
       "       [6.95766568e+01, 4.26180961e+04, 4.24938416e+04],\n",
       "       [8.03651970e+01, 4.38024688e+04, 4.32927216e+04],\n",
       "       [4.32199242e+01, 6.16318414e+04, 6.15481640e+04],\n",
       "       [7.92208670e+01, 6.66927992e+04, 6.63144220e+04],\n",
       "       [5.59881090e+01, 6.63540242e+04, 6.58510185e+04],\n",
       "       [8.06315288e+01, 3.05367441e+04, 3.02847188e+04],\n",
       "       [3.82558234e+01, 6.60485773e+04, 6.56516664e+04],\n",
       "       [6.07256292e+01, 6.89829984e+04, 6.84585243e+04],\n",
       "       [8.35150062e+01, 4.26156492e+04, 4.25308346e+04],\n",
       "       [4.87936311e+01, 6.44680680e+04, 6.41703061e+04],\n",
       "       [1.59336280e+01, 2.61479676e+04, 2.61334828e+04],\n",
       "       [3.16702878e+01, 2.92938434e+04, 2.93632656e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 2.62830375e+04],\n",
       "       [0.00000000e+00, 2.63203551e+04, 2.64023772e+04],\n",
       "       [6.79476873e+01, 2.68399750e+04, 2.68842846e+04],\n",
       "       [7.12360856e+01, 6.39538930e+04, 6.37260531e+04],\n",
       "       [4.76253178e+01, 2.65251309e+04, 2.61600716e+04],\n",
       "       [0.00000000e+00, 2.59750844e+04, 2.61780511e+04],\n",
       "       [4.86829217e+01, 3.06474371e+04, 3.06248147e+04],\n",
       "       [6.69417893e+01, 6.87852469e+04, 6.87230052e+04],\n",
       "       [3.76259124e+01, 2.92245180e+04, 2.91558978e+04],\n",
       "       [4.35444760e+01, 3.00922629e+04, 3.00874089e+04],\n",
       "       [3.01924151e+01, 2.92578430e+04, 2.93664394e+04],\n",
       "       [4.60782472e+01, 4.32791133e+04, 4.33926524e+04],\n",
       "       [4.06393363e+01, 6.18528719e+04, 6.12598260e+04],\n",
       "       [4.78394505e+01, 6.62514109e+04, 6.63301193e+04],\n",
       "       [5.70237543e+01, 6.81944852e+04, 6.79329275e+04],\n",
       "       [6.49020729e+01, 2.77700234e+04, 2.77320067e+04],\n",
       "       [4.36579241e+01, 4.26317469e+04, 4.31299804e+04],\n",
       "       [5.41672807e+01, 2.93183094e+04, 2.94189549e+04],\n",
       "       [7.87685922e+01, 6.67264930e+04, 6.70425756e+04],\n",
       "       [6.12794244e+01, 2.72320000e+04, 2.75405982e+04],\n",
       "       [6.58185048e+01, 4.28732680e+04, 4.22953913e+04],\n",
       "       [8.03939852e+01, 3.04574184e+04, 3.04334856e+04],\n",
       "       [4.43697622e+01, 2.99234270e+04, 2.99622110e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 2.65082168e+04],\n",
       "       [5.54243517e+01, 4.33774211e+04, 4.31303981e+04],\n",
       "       [4.83445923e+01, 7.01562844e+04, 6.99097248e+04],\n",
       "       [5.87955433e+01, 2.59192445e+04, 2.61044533e+04],\n",
       "       [6.70958898e+01, 3.87429422e+04, 3.89906936e+04],\n",
       "       [9.05958658e+01, 5.19326891e+04, 5.15965121e+04],\n",
       "       [4.73426714e+01, 2.59253980e+04, 2.59048956e+04],\n",
       "       [4.60592283e+01, 6.27531602e+04, 6.24916971e+04],\n",
       "       [4.14189903e+01, 2.58072816e+04, 2.56574818e+04],\n",
       "       [3.98244088e+01, 6.48230180e+04, 6.52232279e+04],\n",
       "       [5.18394701e+01, 3.71466023e+04, 3.72639156e+04],\n",
       "       [3.99444024e+01, 2.91958898e+04, 2.91088390e+04],\n",
       "       [3.85709149e+01, 2.95231242e+04, 2.94797094e+04],\n",
       "       [5.84956644e+01, 4.26633359e+04, 4.30183517e+04],\n",
       "       [7.17161722e+01, 7.08414734e+04, 7.09906679e+04],\n",
       "       [4.07458430e+01, 4.17668977e+04, 4.18425682e+04],\n",
       "       [8.27137667e+01, 3.48555930e+04, 3.47559994e+04],\n",
       "       [9.12357856e+01, 3.42394648e+04, 3.39349629e+04],\n",
       "       [9.16296854e+01, 4.08408438e+04, 4.13516572e+04],\n",
       "       [6.12387338e+01, 2.75637898e+04, 2.78323148e+04],\n",
       "       [4.12855213e+01, 6.38092594e+04, 6.38179475e+04],\n",
       "       [5.36539148e+01, 7.01110234e+04, 6.98074627e+04],\n",
       "       [6.98622021e+01, 5.14360883e+04, 5.15257576e+04],\n",
       "       [4.80250011e+01, 4.30440742e+04, 4.30323189e+04],\n",
       "       [4.30195813e+01, 2.66303949e+04, 2.65581998e+04],\n",
       "       [6.61895211e+01, 2.77121445e+04, 2.77417120e+04],\n",
       "       [4.64220544e+01, 6.47988281e+04, 6.50383987e+04],\n",
       "       [3.83591519e+01, 6.32618414e+04, 6.27248771e+04],\n",
       "       [5.08329225e+01, 2.59110793e+04, 2.59931823e+04],\n",
       "       [3.78963738e+01, 2.63958734e+04, 2.66587122e+04],\n",
       "       [7.90730994e+01, 5.95503156e+04, 5.96864524e+04],\n",
       "       [6.13092302e+01, 2.76319641e+04, 2.74689232e+04],\n",
       "       [4.69830725e+01, 6.61050680e+04, 6.71526108e+04],\n",
       "       [3.63323164e+01, 2.92621148e+04, 2.92127832e+04],\n",
       "       [9.06621400e+01, 5.20019531e+04, 5.18259664e+04],\n",
       "       [5.18395875e+01, 6.84645000e+04, 6.89804941e+04],\n",
       "       [4.85919545e+01, 2.99915047e+04, 2.99889445e+04],\n",
       "       [5.41780975e+01, 2.68398453e+04, 2.67787340e+04],\n",
       "       [7.66931775e+01, 6.56662547e+04, 6.53215861e+04],\n",
       "       [3.57696404e+01, 6.54885875e+04, 6.51709060e+04],\n",
       "       [4.65442453e+01, 3.05493406e+04, 3.04264745e+04],\n",
       "       [3.19586812e+01, 2.92784547e+04, 2.92993348e+04],\n",
       "       [3.98284757e+01, 6.35532492e+04, 6.36939155e+04],\n",
       "       [4.87337129e+01, 2.69490441e+04, 2.69969197e+04],\n",
       "       [4.75378884e+01, 6.56960398e+04, 6.57495099e+04],\n",
       "       [4.99676110e+01, 3.05956590e+04, 3.04114102e+04],\n",
       "       [8.42924719e+01, 4.34532914e+04, 4.30760903e+04],\n",
       "       [5.87216517e+01, 3.65510758e+04, 3.65921381e+04],\n",
       "       [7.07174634e+01, 3.49678977e+04, 3.49633055e+04],\n",
       "       [7.89995534e+01, 4.31274789e+04, 4.33441907e+04],\n",
       "       [5.29535769e+01, 4.32664117e+04, 4.32345506e+04],\n",
       "       [4.62761832e+01, 3.02459090e+04, 3.02946926e+04],\n",
       "       [5.17131809e+01, 3.02259699e+04, 3.03346186e+04],\n",
       "       [4.94193591e+01, 2.70934383e+04, 2.70645261e+04],\n",
       "       [8.31638012e+01, 4.37422789e+04, 4.34550475e+04],\n",
       "       [4.57767003e+01, 6.14545570e+04, 6.15149075e+04],\n",
       "       [4.32570151e+01, 6.53290383e+04, 6.54796668e+04],\n",
       "       [4.73819499e+01, 6.09490516e+04, 6.13284088e+04],\n",
       "       [8.66880937e+01, 3.48168242e+04, 3.47678370e+04],\n",
       "       [4.66144232e+01, 6.65141766e+04, 6.61673895e+04],\n",
       "       [6.08772067e+01, 6.46010375e+04, 6.49856139e+04],\n",
       "       [6.83436545e+01, 3.83134812e+04, 3.84968451e+04],\n",
       "       [6.18969012e+01, 2.60745379e+04, 2.62725333e+04],\n",
       "       [4.06443991e+01, 6.29214625e+04, 6.22125652e+04],\n",
       "       [4.91912501e+01, 4.32826742e+04, 4.33978190e+04],\n",
       "       [7.56649014e+01, 4.29010063e+04, 4.28571747e+04],\n",
       "       [6.92549528e+01, 2.70475301e+04, 2.71848767e+04],\n",
       "       [8.45230535e+01, 5.54833805e+04, 5.68648518e+04],\n",
       "       [1.91465173e+01, 2.73364879e+04, 2.70626704e+04],\n",
       "       [3.92493399e+01, 6.26195648e+04, 6.29199074e+04],\n",
       "       [8.26763872e+01, 3.06579781e+04, 3.07160889e+04],\n",
       "       [6.28862785e+01, 3.68401320e+04, 3.70636130e+04],\n",
       "       [4.24795595e+01, 6.21054891e+04, 6.19253905e+04],\n",
       "       [6.83369311e+01, 6.94325453e+04, 6.84810501e+04],\n",
       "       [4.98644541e+01, 3.76439320e+04, 3.76430430e+04],\n",
       "       [6.61270038e+01, 6.86724078e+04, 6.86678191e+04],\n",
       "       [3.97813607e+01, 3.05001891e+04, 3.03329459e+04],\n",
       "       [5.26562770e+01, 4.22364102e+04, 4.22277507e+04],\n",
       "       [4.05354125e+01, 6.36149930e+04, 6.37844963e+04],\n",
       "       [4.68108294e+01, 2.58765957e+04, 2.60018519e+04],\n",
       "       [2.04147798e+01, 2.86456070e+04, 2.82940762e+04],\n",
       "       [4.77060137e+01, 2.61735656e+04, 2.60965699e+04],\n",
       "       [4.78827581e+01, 6.70043570e+04, 6.80976858e+04],\n",
       "       [7.53214566e+01, 4.28992266e+04, 4.27434262e+04],\n",
       "       [7.92439005e+01, 6.89736313e+04, 6.91757333e+04],\n",
       "       [5.07320842e+01, 4.40632180e+04, 4.38242808e+04],\n",
       "       [0.00000000e+00, 2.58915875e+04, 2.60989702e+04],\n",
       "       [5.23935585e+01, 6.78875312e+04, 6.79410445e+04],\n",
       "       [4.95872861e+01, 4.24302195e+04, 4.27120897e+04],\n",
       "       [9.24812221e+01, 4.95185109e+04, 4.95779559e+04],\n",
       "       [4.12736483e+01, 6.51517758e+04, 6.45458888e+04],\n",
       "       [5.53921521e+01, 4.22898508e+04, 4.20298565e+04],\n",
       "       [5.30055354e+01, 2.73648301e+04, 2.73434459e+04],\n",
       "       [3.49733608e+01, 2.96338785e+04, 2.95420778e+04],\n",
       "       [5.34639976e+01, 3.75079969e+04, 3.73874346e+04]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([X_train]).reshape(-1,3)\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56553777, 0.41114268, 0.082805  ],\n",
       "       [0.58527185, 0.59979477, 0.37670751],\n",
       "       [0.49793023, 0.97028613, 0.96330439],\n",
       "       [0.43605963, 0.88796932, 0.8468105 ],\n",
       "       [0.86141616, 0.6096598 , 0.40333791],\n",
       "       [0.36515427, 0.85308527, 0.76734921],\n",
       "       [0.85660636, 0.42510714, 0.10197558],\n",
       "       [0.9270644 , 0.47888701, 0.18620838],\n",
       "       [0.96072976, 0.48458952, 0.19849244],\n",
       "       [0.19212341, 0.41135387, 0.082091  ],\n",
       "       [0.61016496, 0.96762095, 0.9682262 ],\n",
       "       [0.54502679, 0.86013574, 0.79567563],\n",
       "       [0.4576614 , 0.97517595, 0.95335515],\n",
       "       [0.5287328 , 0.59946913, 0.37528986],\n",
       "       [0.83339567, 0.87091063, 0.78537967],\n",
       "       [0.66132359, 1.        , 0.98737439],\n",
       "       [0.45085635, 0.90396836, 0.86542775],\n",
       "       [0.83727993, 0.42509872, 0.10331786],\n",
       "       [0.66599781, 0.60322627, 0.39604146],\n",
       "       [0.23441379, 0.57115788, 0.32990932],\n",
       "       [0.65649291, 0.61046862, 0.39099412],\n",
       "       [0.74225709, 0.4102039 , 0.08254989],\n",
       "       [0.62032933, 0.3815941 , 0.03612826],\n",
       "       [0.52105245, 0.41144666, 0.08166898],\n",
       "       [0.71790057, 0.40187611, 0.06735251],\n",
       "       [0.        , 0.37582372, 0.03049582],\n",
       "       [0.67744781, 0.96124412, 0.95126489],\n",
       "       [0.72221051, 0.51666481, 0.24184284],\n",
       "       [0.59746327, 0.97828522, 0.96509188],\n",
       "       [0.64262328, 0.52626956, 0.26411245],\n",
       "       [0.84636487, 0.42424796, 0.0952626 ],\n",
       "       [0.68590555, 0.64351534, 0.446611  ],\n",
       "       [0.94198204, 0.48031679, 0.19019418],\n",
       "       [0.42306529, 0.89108769, 0.85242585],\n",
       "       [0.53355158, 0.57619498, 0.34851652],\n",
       "       [0.54613147, 0.64046649, 0.42380368],\n",
       "       [0.71562428, 0.37393189, 0.02425726],\n",
       "       [0.55924164, 0.95961791, 0.9385    ],\n",
       "       [0.52010787, 0.60849239, 0.39267289],\n",
       "       [0.50425806, 0.95682565, 0.93773097],\n",
       "       [0.82547892, 0.85389752, 0.7655873 ],\n",
       "       [0.76585133, 0.55340523, 0.31515624],\n",
       "       [0.4724505 , 0.95697721, 0.92344134],\n",
       "       [0.85653938, 0.49019506, 0.20385084],\n",
       "       [0.41780355, 0.59103915, 0.36280429],\n",
       "       [0.14471667, 0.36480654, 0.01026843],\n",
       "       [0.        , 0.        , 0.01850632],\n",
       "       [0.67250845, 0.49307176, 0.21208542],\n",
       "       [0.        , 0.36042994, 0.00518239],\n",
       "       [0.80812584, 0.80294536, 0.71812099],\n",
       "       [0.86737798, 0.636727  , 0.44640798],\n",
       "       [0.87477169, 0.50391277, 0.23365035],\n",
       "       [0.48709058, 0.38194149, 0.03316846],\n",
       "       [0.60664712, 0.97230837, 0.9459356 ],\n",
       "       [0.68414381, 0.39481772, 0.0567622 ],\n",
       "       [0.77519023, 0.90474275, 0.85470512],\n",
       "       [0.97129124, 0.72648655, 0.56847525],\n",
       "       [0.51063311, 0.42474436, 0.10496457],\n",
       "       [0.60624997, 0.9148379 , 0.87928713],\n",
       "       [0.56348148, 0.42488249, 0.10468294],\n",
       "       [0.52915298, 0.98244693, 0.96848141],\n",
       "       [0.39636108, 0.36071656, 0.0050444 ],\n",
       "       [0.49230582, 0.37165614, 0.0142566 ],\n",
       "       [0.66790425, 0.62996394, 0.43264389],\n",
       "       [0.40221469, 0.95332944, 0.91535351],\n",
       "       [0.62752013, 0.51098635, 0.2405594 ],\n",
       "       [0.53377982, 0.95422576, 0.93252738],\n",
       "       [0.69489033, 0.60479831, 0.37521572],\n",
       "       [0.54699935, 0.51861524, 0.25024472],\n",
       "       [0.57677661, 0.37236223, 0.01921063],\n",
       "       [0.38530522, 0.41605194, 0.08889539],\n",
       "       [0.57915158, 0.95271715, 0.94465216],\n",
       "       [0.8988068 , 0.58536592, 0.36272232],\n",
       "       [0.21502767, 0.37400214, 0.02450399],\n",
       "       [0.89848946, 0.42351309, 0.10949406],\n",
       "       [0.69150083, 0.95626918, 0.94275956],\n",
       "       [0.59551903, 0.51664618, 0.24745281],\n",
       "       [0.57792896, 0.98095385, 0.94898303],\n",
       "       [0.47567184, 0.42714535, 0.10404743],\n",
       "       [0.73827258, 0.72095095, 0.56664589],\n",
       "       [0.88573163, 0.65129876, 0.46355493],\n",
       "       [0.55905597, 0.98274023, 0.97506205],\n",
       "       [0.45340226, 0.91474018, 0.87885971],\n",
       "       [0.58180158, 0.42982863, 0.11007544],\n",
       "       [0.21826941, 0.56200205, 0.32176147],\n",
       "       [0.67241787, 0.52965325, 0.27160828],\n",
       "       [0.83788886, 0.50978898, 0.23993191],\n",
       "       [0.53914926, 0.52510182, 0.25944528],\n",
       "       [0.93371493, 0.43800786, 0.13343383],\n",
       "       [0.64933245, 0.58893698, 0.36874189],\n",
       "       [0.82093783, 0.94786412, 0.92468786],\n",
       "       [0.86059781, 0.62242556, 0.42643485],\n",
       "       [0.95789626, 0.70424485, 0.54339473],\n",
       "       [0.64318815, 0.4281931 , 0.11244503],\n",
       "       [0.21601314, 0.56684633, 0.32565079],\n",
       "       [0.65472502, 0.6368595 , 0.44216425],\n",
       "       [0.48097344, 0.5700688 , 0.34267082],\n",
       "       [0.50777753, 0.59362201, 0.36785666],\n",
       "       [0.45072072, 0.41747585, 0.09327909],\n",
       "       [0.6160327 , 0.95709768, 0.93875899],\n",
       "       [0.39626915, 0.37627404, 0.0306516 ],\n",
       "       [0.50737208, 0.36245793, 0.00624055],\n",
       "       [0.54670245, 0.36841593, 0.01561321],\n",
       "       [0.53735584, 0.51430861, 0.24169551],\n",
       "       [0.9041899 , 0.67949788, 0.50136044],\n",
       "       [0.39976296, 0.59153668, 0.36869013],\n",
       "       [0.82671253, 0.42493003, 0.09733145],\n",
       "       [0.89058377, 0.7264243 , 0.57562691],\n",
       "       [0.14590676, 0.36555468, 0.01095034],\n",
       "       [0.38680988, 0.91451806, 0.86397607],\n",
       "       [0.18933347, 0.39160549, 0.04153252],\n",
       "       [0.19539648, 0.57512026, 0.33884311],\n",
       "       [0.54686819, 0.97797935, 0.97159145],\n",
       "       [0.72104814, 0.96500988, 0.9475836 ],\n",
       "       [0.54621657, 0.59800468, 0.38356879],\n",
       "       [0.68873771, 0.51501757, 0.24538245],\n",
       "       [0.3975849 , 0.95141296, 0.93535924],\n",
       "       [0.59186594, 0.51226705, 0.2405448 ],\n",
       "       [0.39623849, 0.37267984, 0.0267016 ],\n",
       "       [0.73427461, 0.5941118 , 0.37417272],\n",
       "       [0.66126105, 0.97615331, 0.97611367],\n",
       "       [0.60759918, 0.38521969, 0.04042243],\n",
       "       [0.48770773, 0.85260122, 0.80330967],\n",
       "       [0.51016764, 0.59576399, 0.36931727],\n",
       "       [0.71846692, 0.8754501 , 0.8281658 ],\n",
       "       [0.59530401, 0.3690505 , 0.01577457],\n",
       "       [0.86188795, 0.48912768, 0.20249735],\n",
       "       [0.80311356, 0.51429529, 0.24350724],\n",
       "       [0.70569946, 0.49049   , 0.20818612],\n",
       "       [0.85673845, 0.41822603, 0.0898596 ],\n",
       "       [0.63722657, 0.59843741, 0.37390646],\n",
       "       [0.06854202, 0.36502136, 0.01591451],\n",
       "       [0.70279586, 0.37003062, 0.0175063 ],\n",
       "       [0.57514462, 0.98204984, 0.97825038],\n",
       "       [0.51472734, 0.59539561, 0.37115187],\n",
       "       [0.95874735, 0.71505997, 0.55655682],\n",
       "       [1.        , 0.45149043, 0.1538018 ],\n",
       "       [0.56915507, 0.94798156, 0.92856417],\n",
       "       [0.        , 0.36555664, 0.00929251],\n",
       "       [0.30305781, 0.35981396, 0.00128926],\n",
       "       [0.77344871, 0.39568849, 0.06610297],\n",
       "       [0.63580777, 0.40869543, 0.08115957],\n",
       "       [0.55645481, 0.41101853, 0.0819446 ],\n",
       "       [0.43375983, 0.88358253, 0.83764195],\n",
       "       [0.42870291, 0.58862949, 0.36474768],\n",
       "       [0.80571414, 0.497578  , 0.2222935 ],\n",
       "       [0.51504178, 0.59595731, 0.38143751],\n",
       "       [0.73355757, 0.99892588, 1.        ],\n",
       "       [0.68548794, 0.96226976, 0.94676473],\n",
       "       [0.49553621, 0.86432377, 0.7987862 ],\n",
       "       [0.42088604, 0.42205895, 0.09933625],\n",
       "       [0.611245  , 0.36965276, 0.02040888],\n",
       "       [0.        , 0.38617322, 0.05236603],\n",
       "       [0.61845159, 0.37503166, 0.02222518],\n",
       "       [0.42570698, 0.58867157, 0.35922627],\n",
       "       [0.78482652, 0.97743959, 0.97401003],\n",
       "       [0.        , 0.        , 0.0112444 ],\n",
       "       [0.89251918, 0.66707547, 0.48718738],\n",
       "       [0.57568846, 0.38533192, 0.03766391],\n",
       "       [0.85830493, 0.42580239, 0.10641284],\n",
       "       [0.48694093, 0.42472878, 0.10334496],\n",
       "       [0.15326447, 0.36627525, 0.01455813],\n",
       "       [0.46255814, 0.56276036, 0.33297348],\n",
       "       [0.722791  , 0.59607552, 0.37028628],\n",
       "       [0.8348668 , 0.61264068, 0.38785625],\n",
       "       [0.44898639, 0.86201017, 0.78935277],\n",
       "       [0.82297903, 0.93279496, 0.89417828],\n",
       "       [0.58162756, 0.9280567 , 0.88398653],\n",
       "       [0.83763356, 0.4271004 , 0.10176798],\n",
       "       [0.39741726, 0.92378459, 0.87960213],\n",
       "       [0.63084287, 0.9648267 , 0.94133405],\n",
       "       [0.86758831, 0.5960413 , 0.37109988],\n",
       "       [0.50688836, 0.90167889, 0.8470222 ],\n",
       "       [0.1655251 , 0.36571703, 0.01046881],\n",
       "       [0.32900401, 0.40971664, 0.08150223],\n",
       "       [0.        , 0.        , 0.013758  ],\n",
       "       [0.        , 0.36812812, 0.01638267],\n",
       "       [0.70586859, 0.37539575, 0.02698138],\n",
       "       [0.74002983, 0.89448741, 0.83725163],\n",
       "       [0.49475144, 0.3709922 , 0.01105358],\n",
       "       [0.        , 0.36329901, 0.01144901],\n",
       "       [0.50573826, 0.4286486 , 0.1092478 ],\n",
       "       [0.69541891, 0.96206086, 0.94715085],\n",
       "       [0.39087349, 0.40874702, 0.07694154],\n",
       "       [0.45235797, 0.42088369, 0.0974285 ],\n",
       "       [0.31365126, 0.40921312, 0.08157203],\n",
       "       [0.47867983, 0.6053208 , 0.39005405],\n",
       "       [0.42217818, 0.8651016 , 0.78301128],\n",
       "       [0.49697593, 0.92662151, 0.89452351],\n",
       "       [0.59238627, 0.9537982 , 0.92977447],\n",
       "       [0.67422949, 0.38840382, 0.04562555],\n",
       "       [0.45353651, 0.59626645, 0.38427704],\n",
       "       [0.56271204, 0.41005883, 0.08272702],\n",
       "       [0.81828061, 0.93326622, 0.91019274],\n",
       "       [0.63659592, 0.38087879, 0.04141585],\n",
       "       [0.68374976, 0.59964447, 0.36592171],\n",
       "       [0.83516586, 0.42599091, 0.10503985],\n",
       "       [0.46093138, 0.41852227, 0.09467499],\n",
       "       [0.        , 0.        , 0.01871043],\n",
       "       [0.57577102, 0.60669578, 0.38428623],\n",
       "       [0.50222356, 0.98123679, 0.97325067],\n",
       "       [0.61079234, 0.36251801, 0.00983036],\n",
       "       [0.69701976, 0.54187591, 0.29324067],\n",
       "       [0.9411472 , 0.72635354, 0.57048359],\n",
       "       [0.49181519, 0.36260407, 0.00544143],\n",
       "       [0.47848225, 0.87769343, 0.81010413],\n",
       "       [0.43027755, 0.36095205, 0.        ],\n",
       "       [0.41371238, 0.90664337, 0.87017937],\n",
       "       [0.53852979, 0.5195488 , 0.25526321],\n",
       "       [0.41495892, 0.40834662, 0.07590656],\n",
       "       [0.40069057, 0.41292346, 0.08406321],\n",
       "       [0.60767708, 0.59670827, 0.38182196],\n",
       "       [0.74501716, 0.99082015, 0.99702412],\n",
       "       [0.42328462, 0.58417029, 0.35596266],\n",
       "       [0.85926471, 0.48750573, 0.20010598],\n",
       "       [0.94779495, 0.47888829, 0.18204872],\n",
       "       [0.95188695, 0.57121809, 0.34516593],\n",
       "       [0.63617321, 0.38551935, 0.04783165],\n",
       "       [0.42889102, 0.8924645 , 0.83927269],\n",
       "       [0.55737899, 0.98060376, 0.97100159],\n",
       "       [0.72575736, 0.71940786, 0.56892747],\n",
       "       [0.49890351, 0.60203344, 0.38212915],\n",
       "       [0.44690515, 0.37246447, 0.01980972],\n",
       "       [0.68760403, 0.3875943 , 0.045839  ],\n",
       "       [0.48225144, 0.90630504, 0.86611438],\n",
       "       [0.39849068, 0.88480807, 0.81523251],\n",
       "       [0.52807336, 0.3624038 , 0.00738314],\n",
       "       [0.39368315, 0.36918435, 0.02202031],\n",
       "       [0.82144396, 0.83289703, 0.74840768],\n",
       "       [0.63690556, 0.38647286, 0.03983948],\n",
       "       [0.48807953, 0.92457469, 0.91261277],\n",
       "       [0.37743508, 0.40927287, 0.07819263],\n",
       "       [0.94183569, 0.7273223 , 0.57553004],\n",
       "       [0.53853101, 0.95757475, 0.95281387],\n",
       "       [0.50479326, 0.41947444, 0.09526294],\n",
       "       [0.56282441, 0.37539394, 0.02465998],\n",
       "       [0.79672035, 0.91843726, 0.87234259],\n",
       "       [0.37158977, 0.91595232, 0.86902864],\n",
       "       [0.48352081, 0.42727658, 0.10488565],\n",
       "       [0.33199996, 0.40950141, 0.08009619],\n",
       "       [0.41375463, 0.88888383, 0.83654482],\n",
       "       [0.5062659 , 0.37692124, 0.02945859],\n",
       "       [0.49384318, 0.91885384, 0.88175402],\n",
       "       [0.51908414, 0.42792441, 0.10455434],\n",
       "       [0.87566495, 0.60775694, 0.38309182],\n",
       "       [0.61002473, 0.5112195 , 0.24048864],\n",
       "       [0.73464216, 0.48907647, 0.20466531],\n",
       "       [0.82067993, 0.60319998, 0.38898822],\n",
       "       [0.55010359, 0.60514315, 0.38657688],\n",
       "       [0.48073607, 0.42303265, 0.10198734],\n",
       "       [0.53721784, 0.42275377, 0.10286544],\n",
       "       [0.51338868, 0.3789408 , 0.03094548],\n",
       "       [0.86393985, 0.61179884, 0.39142632],\n",
       "       [0.47554724, 0.85953059, 0.78862135],\n",
       "       [0.44937171, 0.9137208 , 0.87581929],\n",
       "       [0.49222323, 0.85246037, 0.78451964],\n",
       "       [0.90055166, 0.48696349, 0.20036633],\n",
       "       [0.48424985, 0.93029667, 0.89094455],\n",
       "       [0.63241752, 0.90353866, 0.86495347],\n",
       "       [0.70998206, 0.53586928, 0.28237933],\n",
       "       [0.64301053, 0.36469001, 0.01352698],\n",
       "       [0.42223077, 0.88004738, 0.80396511],\n",
       "       [0.51101899, 0.60537061, 0.39016768],\n",
       "       [0.78603819, 0.60003243, 0.37827716],\n",
       "       [0.719449  , 0.37829871, 0.03359238],\n",
       "       [0.87806033, 0.77601508, 0.68635151],\n",
       "       [0.19890192, 0.3823402 , 0.03090466],\n",
       "       [0.40773832, 0.87582491, 0.81952187],\n",
       "       [0.8588764 , 0.42879603, 0.11125521],\n",
       "       [0.65328859, 0.51526237, 0.2508579 ],\n",
       "       [0.44129518, 0.86863481, 0.79764921],\n",
       "       [0.70991221, 0.97111426, 0.94182947],\n",
       "       [0.51801251, 0.52650467, 0.26360145],\n",
       "       [0.68695458, 0.96048264, 0.94593713],\n",
       "       [0.41326518, 0.42658912, 0.10282865],\n",
       "       [0.54701511, 0.5907371 , 0.36443408],\n",
       "       [0.42109858, 0.8897474 , 0.83853698],\n",
       "       [0.48629019, 0.3619215 , 0.00757382],\n",
       "       [0.21207715, 0.40065012, 0.05798728],\n",
       "       [0.49558974, 0.36607505, 0.00965697],\n",
       "       [0.49742583, 0.93715255, 0.93339804],\n",
       "       [0.78247034, 0.60000754, 0.37577546],\n",
       "       [0.82321831, 0.96469569, 0.95710781],\n",
       "       [0.52702581, 0.61628764, 0.39954696],\n",
       "       [0.        , 0.36213118, 0.00970976],\n",
       "       [0.54428589, 0.94950501, 0.92995299],\n",
       "       [0.51513317, 0.5934478 , 0.37508627],\n",
       "       [0.96073306, 0.69258778, 0.52608898],\n",
       "       [0.42876767, 0.91124153, 0.85528248],\n",
       "       [0.57543651, 0.59148454, 0.36008174],\n",
       "       [0.55064335, 0.38273661, 0.03707983],\n",
       "       [0.36331769, 0.41447252, 0.08543489],\n",
       "       [0.55540605, 0.52460342, 0.25797979]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_X = MinMaxScaler(feature_range=(0,1))\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.16766309e-01, 3.75520880e-01, 2.31998801e-02],\n",
       "       [8.28898121e-01, 7.25422806e-01, 5.71775170e-01],\n",
       "       [5.08513408e-01, 4.18113562e-01, 9.46411860e-02],\n",
       "       [7.51214367e-01, 4.29725374e-01, 1.11706024e-01],\n",
       "       [1.50573586e-01, 3.65878267e-01, 1.25659081e-02],\n",
       "       [7.06027469e-01, 9.42548625e-01, 9.25019897e-01],\n",
       "       [4.40304909e-01, 3.61364902e-01, 4.66683388e-03],\n",
       "       [4.07811450e-01, 9.43817851e-01, 9.04074626e-01],\n",
       "       [6.24466008e-01, 6.01655033e-01, 3.77772976e-01],\n",
       "       [0.00000000e+00, 3.60115388e-01, 1.50077500e-03],\n",
       "       [6.85745462e-01, 3.86368934e-01, 4.33413558e-02],\n",
       "       [9.44227002e-01, 4.74299913e-01, 1.70393530e-01],\n",
       "       [7.61756752e-01, 7.26913607e-01, 5.90896126e-01],\n",
       "       [5.49135251e-01, 9.26128422e-01, 9.03746695e-01],\n",
       "       [5.87811929e-01, 9.67920249e-01, 9.41741448e-01],\n",
       "       [5.33622243e-01, 6.10979772e-01, 3.95825590e-01],\n",
       "       [5.09407082e-01, 9.71725162e-01, 9.59440784e-01],\n",
       "       [6.08255064e-01, 3.70582415e-01, 1.68906257e-02],\n",
       "       [4.59554031e-01, 6.05255930e-01, 3.84002511e-01],\n",
       "       [9.72905100e-01, 4.63345999e-01, 1.64842635e-01],\n",
       "       [9.46232405e-01, 4.77106544e-01, 1.75412060e-01],\n",
       "       [4.83090370e-01, 6.10578022e-01, 3.89039532e-01],\n",
       "       [3.13956202e-01, 4.06973886e-01, 7.61054852e-02],\n",
       "       [4.64315707e-01, 9.53031583e-01, 9.43351147e-01],\n",
       "       [6.22570172e-01, 9.44018862e-01, 9.29075917e-01],\n",
       "       [9.32077757e-01, 7.19828710e-01, 5.61686475e-01],\n",
       "       [4.04971451e-01, 4.10293349e-01, 8.03828454e-02],\n",
       "       [4.93774611e-01, 8.62095989e-01, 8.16860223e-01],\n",
       "       [5.76312948e-01, 3.71484694e-01, 2.28008504e-02],\n",
       "       [6.34439399e-01, 5.91723873e-01, 3.67396613e-01],\n",
       "       [6.57245955e-01, 3.88334187e-01, 4.31257409e-02],\n",
       "       [6.64836167e-01, 3.72053669e-01, 1.93011454e-02],\n",
       "       [8.03178284e-01, 4.07882589e-01, 8.10025526e-02],\n",
       "       [4.90909483e-01, 4.28316661e-01, 1.06835811e-01],\n",
       "       [4.84559221e-01, 4.10802264e-01, 8.28555884e-02],\n",
       "       [0.00000000e+00, 3.63907670e-01, 8.11023090e-03],\n",
       "       [5.20439434e-01, 4.07053942e-01, 7.65651035e-02],\n",
       "       [5.86373399e-01, 3.89620077e-01, 5.14680626e-02],\n",
       "       [8.33853345e-01, 7.43083049e-01, 6.24331233e-01],\n",
       "       [5.36482627e-01, 4.12138123e-01, 8.26232942e-02],\n",
       "       [5.74824870e-01, 5.94681699e-01, 3.69187057e-01],\n",
       "       [6.37778161e-01, 9.41764138e-01, 9.36369936e-01],\n",
       "       [5.31029873e-01, 6.15789338e-01, 4.00755966e-01],\n",
       "       [7.30929425e-01, 7.21532903e-01, 5.65006462e-01],\n",
       "       [8.58829099e-01, 4.25396226e-01, 1.00312963e-01],\n",
       "       [5.67544208e-01, 9.68523360e-01, 9.66806447e-01],\n",
       "       [5.65309418e-01, 6.12950451e-01, 4.01224021e-01],\n",
       "       [6.54365001e-01, 6.24481791e-01, 4.23730226e-01],\n",
       "       [6.06680571e-01, 6.00827047e-01, 3.78931284e-01],\n",
       "       [5.32364831e-01, 4.12796567e-01, 8.26359492e-02],\n",
       "       [6.33264448e-01, 5.26118052e-01, 2.61232237e-01],\n",
       "       [5.03756098e-01, 6.16769034e-01, 3.90661289e-01],\n",
       "       [5.62619347e-01, 5.85177551e-01, 3.61596951e-01],\n",
       "       [6.48741309e-01, 5.16391422e-01, 2.46312513e-01],\n",
       "       [5.04340329e-01, 3.62225024e-01, 6.74146909e-03],\n",
       "       [3.94201716e-01, 4.10334314e-01, 8.08553630e-02],\n",
       "       [3.24576530e-01, 4.09850678e-01, 8.08574114e-02],\n",
       "       [7.32285210e-01, 4.05789360e-01, 7.61389060e-02],\n",
       "       [4.92339704e-01, 3.61497740e-01, 5.94727435e-03],\n",
       "       [1.57945632e-01, 3.66090156e-01, 1.62850973e-02],\n",
       "       [5.35408971e-01, 8.91245504e-01, 8.39442574e-01],\n",
       "       [8.91725103e-01, 8.87206580e-01, 8.36425277e-01],\n",
       "       [0.00000000e+00, 3.59346565e-01, 4.06058968e-04],\n",
       "       [6.27294818e-01, 3.68621953e-01, 1.56951347e-02],\n",
       "       [4.18232316e-01, 8.80490345e-01, 8.16065093e-01],\n",
       "       [6.69942120e-01, 8.87864106e-01, 8.42231568e-01],\n",
       "       [4.15497175e-01, 4.09835730e-01, 7.97177711e-02],\n",
       "       [7.37221948e-01, 3.89007241e-01, 4.72566653e-02],\n",
       "       [5.43789339e-01, 6.28918946e-01, 4.08519731e-01],\n",
       "       [5.95715970e-01, 5.13432590e-01, 2.33660827e-01],\n",
       "       [1.88253165e-01, 3.66791745e-01, 1.97571113e-02],\n",
       "       [5.50683124e-01, 5.20443469e-01, 2.59169775e-01],\n",
       "       [5.94025048e-01, 3.73357298e-01, 2.48074695e-02],\n",
       "       [5.27198211e-01, 9.79162190e-01, 9.63920209e-01]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([X_test]).reshape(-1,3)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08904136],\n",
       "       [0.36353548],\n",
       "       [0.91768751],\n",
       "       [0.83119959],\n",
       "       [0.42025369],\n",
       "       [0.70868489],\n",
       "       [0.11023798],\n",
       "       [0.19489867],\n",
       "       [0.21440773],\n",
       "       [0.08447337],\n",
       "       [0.95825033],\n",
       "       [0.75938006],\n",
       "       [0.87713838],\n",
       "       [0.35343174],\n",
       "       [0.79307275],\n",
       "       [0.9232124 ],\n",
       "       [0.81174588],\n",
       "       [0.11090367],\n",
       "       [0.41308358],\n",
       "       [0.30639894],\n",
       "       [0.38502005],\n",
       "       [0.10081713],\n",
       "       [0.04941755],\n",
       "       [0.08598053],\n",
       "       [0.09432836],\n",
       "       [0.06604295],\n",
       "       [0.92302003],\n",
       "       [0.236634  ],\n",
       "       [0.92825262],\n",
       "       [0.26189212],\n",
       "       [0.11096648],\n",
       "       [0.44251998],\n",
       "       [0.19835057],\n",
       "       [0.82978482],\n",
       "       [0.3521002 ],\n",
       "       [0.36916113],\n",
       "       [0.0427468 ],\n",
       "       [0.88511458],\n",
       "       [0.38765778],\n",
       "       [0.88449899],\n",
       "       [0.7693352 ],\n",
       "       [0.35094212],\n",
       "       [0.85178293],\n",
       "       [0.20631316],\n",
       "       [0.34339444],\n",
       "       [0.01968864],\n",
       "       [0.02749786],\n",
       "       [0.21895738],\n",
       "       [0.02430693],\n",
       "       [0.7519844 ],\n",
       "       [0.47179099],\n",
       "       [0.25357053],\n",
       "       [0.0332662 ],\n",
       "       [0.90207028],\n",
       "       [0.07422975],\n",
       "       [0.8544121 ],\n",
       "       [0.56259217],\n",
       "       [0.11390235],\n",
       "       [0.8718158 ],\n",
       "       [0.10911728],\n",
       "       [0.92446284],\n",
       "       [0.02223813],\n",
       "       [0.01331509],\n",
       "       [0.43774428],\n",
       "       [0.80672237],\n",
       "       [0.23860241],\n",
       "       [0.88330578],\n",
       "       [0.33557792],\n",
       "       [0.25306245],\n",
       "       [0.0228329 ],\n",
       "       [0.08376859],\n",
       "       [0.92235226],\n",
       "       [0.38780182],\n",
       "       [0.02142971],\n",
       "       [0.16534725],\n",
       "       [0.93850341],\n",
       "       [0.25604067],\n",
       "       [0.83789137],\n",
       "       [0.10808681],\n",
       "       [0.55108475],\n",
       "       [0.48269732],\n",
       "       [0.92943882],\n",
       "       [0.8606822 ],\n",
       "       [0.11167398],\n",
       "       [0.30824068],\n",
       "       [0.28225912],\n",
       "       [0.24989952],\n",
       "       [0.25701689],\n",
       "       [0.18236058],\n",
       "       [0.34989164],\n",
       "       [0.91519952],\n",
       "       [0.45876788],\n",
       "       [0.55875267],\n",
       "       [0.13174631],\n",
       "       [0.31123032],\n",
       "       [0.44792872],\n",
       "       [0.35386271],\n",
       "       [0.37123625],\n",
       "       [0.09618254],\n",
       "       [0.90153538],\n",
       "       [0.04167285],\n",
       "       [0.01529655],\n",
       "       [0.02483392],\n",
       "       [0.22226566],\n",
       "       [0.51292474],\n",
       "       [0.33595827],\n",
       "       [0.10660239],\n",
       "       [0.55667894],\n",
       "       [0.01765012],\n",
       "       [0.82049998],\n",
       "       [0.01850765],\n",
       "       [0.29934181],\n",
       "       [0.9479221 ],\n",
       "       [0.92033624],\n",
       "       [0.39036244],\n",
       "       [0.26539448],\n",
       "       [0.91071279],\n",
       "       [0.22938066],\n",
       "       [0.04454453],\n",
       "       [0.37272544],\n",
       "       [0.95145461],\n",
       "       [0.05502282],\n",
       "       [0.80818312],\n",
       "       [0.3635151 ],\n",
       "       [0.85776529],\n",
       "       [0.02200703],\n",
       "       [0.20699845],\n",
       "       [0.24815641],\n",
       "       [0.21453937],\n",
       "       [0.11239453],\n",
       "       [0.37380641],\n",
       "       [0.01813411],\n",
       "       [0.02370213],\n",
       "       [0.96348454],\n",
       "       [0.35463164],\n",
       "       [0.56337797],\n",
       "       [0.1949082 ],\n",
       "       [0.8887353 ],\n",
       "       [0.01543891],\n",
       "       [0.01399578],\n",
       "       [0.09911401],\n",
       "       [0.09605085],\n",
       "       [0.08860004],\n",
       "       [0.8071835 ],\n",
       "       [0.35700274],\n",
       "       [0.24061493],\n",
       "       [0.36685492],\n",
       "       [0.96479807],\n",
       "       [0.90010023],\n",
       "       [0.78752356],\n",
       "       [0.09795128],\n",
       "       [0.0387909 ],\n",
       "       [0.10151412],\n",
       "       [0.02956409],\n",
       "       [0.34437897],\n",
       "       [0.96656547],\n",
       "       [0.01622496],\n",
       "       [0.5174276 ],\n",
       "       [0.03569774],\n",
       "       [0.11389856],\n",
       "       [0.10704917],\n",
       "       [0.02086188],\n",
       "       [0.34753595],\n",
       "       [0.36994522],\n",
       "       [0.38737484],\n",
       "       [0.74353981],\n",
       "       [0.9001811 ],\n",
       "       [0.85799222],\n",
       "       [0.11530493],\n",
       "       [0.79847415],\n",
       "       [0.89244895],\n",
       "       [0.37833242],\n",
       "       [0.79832661],\n",
       "       [0.01934521],\n",
       "       [0.08752483],\n",
       "       [0.01436924],\n",
       "       [0.03523258],\n",
       "       [0.04109597],\n",
       "       [0.80629931],\n",
       "       [0.01473564],\n",
       "       [0.02448952],\n",
       "       [0.10791575],\n",
       "       [0.91733944],\n",
       "       [0.08095585],\n",
       "       [0.09914828],\n",
       "       [0.0867387 ],\n",
       "       [0.36905696],\n",
       "       [0.69054202],\n",
       "       [0.8415698 ],\n",
       "       [0.90453724],\n",
       "       [0.0505213 ],\n",
       "       [0.37545855],\n",
       "       [0.09179387],\n",
       "       [0.90432938],\n",
       "       [0.07004911],\n",
       "       [0.33988483],\n",
       "       [0.1132581 ],\n",
       "       [0.09904019],\n",
       "       [0.02807886],\n",
       "       [0.36221706],\n",
       "       [0.93691909],\n",
       "       [0.02873529],\n",
       "       [0.30917101],\n",
       "       [0.55542614],\n",
       "       [0.01397246],\n",
       "       [0.79062266],\n",
       "       [0.        ],\n",
       "       [0.86966381],\n",
       "       [0.26204936],\n",
       "       [0.07385356],\n",
       "       [0.08748423],\n",
       "       [0.39658477],\n",
       "       [1.        ],\n",
       "       [0.341879  ],\n",
       "       [0.19969743],\n",
       "       [0.19565235],\n",
       "       [0.39477588],\n",
       "       [0.06788473],\n",
       "       [0.79194296],\n",
       "       [0.92193556],\n",
       "       [0.55446818],\n",
       "       [0.36445937],\n",
       "       [0.01604351],\n",
       "       [0.05855876],\n",
       "       [0.80652185],\n",
       "       [0.77569921],\n",
       "       [0.02248585],\n",
       "       [0.05352031],\n",
       "       [0.77790736],\n",
       "       [0.04702041],\n",
       "       [0.93479482],\n",
       "       [0.08162288],\n",
       "       [0.56597959],\n",
       "       [0.9475023 ],\n",
       "       [0.09660433],\n",
       "       [0.02931873],\n",
       "       [0.8714961 ],\n",
       "       [0.81622608],\n",
       "       [0.10614334],\n",
       "       [0.08487866],\n",
       "       [0.80028011],\n",
       "       [0.0354554 ],\n",
       "       [0.87793771],\n",
       "       [0.09905258],\n",
       "       [0.39656953],\n",
       "       [0.23837327],\n",
       "       [0.20606307],\n",
       "       [0.39973352],\n",
       "       [0.38146658],\n",
       "       [0.10451777],\n",
       "       [0.10959357],\n",
       "       [0.03546934],\n",
       "       [0.38849575],\n",
       "       [0.75720158],\n",
       "       [0.80583109],\n",
       "       [0.78728122],\n",
       "       [0.20399448],\n",
       "       [0.76689212],\n",
       "       [0.87413359],\n",
       "       [0.29869418],\n",
       "       [0.03017557],\n",
       "       [0.75176651],\n",
       "       [0.39304916],\n",
       "       [0.3739921 ],\n",
       "       [0.05886991],\n",
       "       [0.77924616],\n",
       "       [0.0194811 ],\n",
       "       [0.79295961],\n",
       "       [0.12507678],\n",
       "       [0.25697173],\n",
       "       [0.74352033],\n",
       "       [0.90489737],\n",
       "       [0.26493351],\n",
       "       [0.90473027],\n",
       "       [0.10398887],\n",
       "       [0.36436929],\n",
       "       [0.75361853],\n",
       "       [0.01355906],\n",
       "       [0.03134119],\n",
       "       [0.01683844],\n",
       "       [0.93540047],\n",
       "       [0.36510236],\n",
       "       [0.97997544],\n",
       "       [0.39648793],\n",
       "       [0.02813017],\n",
       "       [0.88780326],\n",
       "       [0.38583617],\n",
       "       [0.55641842],\n",
       "       [0.80534192],\n",
       "       [0.33809941],\n",
       "       [0.04731394],\n",
       "       [0.08482188],\n",
       "       [0.25232263]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_train).reshape(-1,1)\n",
    "scaler_Y = MinMaxScaler(feature_range=(0,1))\n",
    "y_train = scaler_Y.fit_transform(y_train)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.95678016e-02],\n",
       "       [ 5.45531240e-01],\n",
       "       [ 1.02708637e-01],\n",
       "       [ 1.17170881e-01],\n",
       "       [ 1.84682398e-02],\n",
       "       [ 9.65874878e-01],\n",
       "       [ 1.23241153e-02],\n",
       "       [ 8.46731139e-01],\n",
       "       [ 3.76103652e-01],\n",
       "       [-7.92526080e-04],\n",
       "       [ 5.80946208e-02],\n",
       "       [ 1.82533224e-01],\n",
       "       [ 6.12671743e-01],\n",
       "       [ 8.92117344e-01],\n",
       "       [ 8.40642893e-01],\n",
       "       [ 3.96847495e-01],\n",
       "       [ 9.51199800e-01],\n",
       "       [ 2.86208054e-02],\n",
       "       [ 3.62041074e-01],\n",
       "       [ 1.87684378e-01],\n",
       "       [ 1.86284691e-01],\n",
       "       [ 3.72561515e-01],\n",
       "       [ 8.09501818e-02],\n",
       "       [ 9.24288560e-01],\n",
       "       [ 8.90530743e-01],\n",
       "       [ 5.53002381e-01],\n",
       "       [ 8.36315241e-02],\n",
       "       [ 8.11097508e-01],\n",
       "       [ 3.64990721e-02],\n",
       "       [ 3.56368145e-01],\n",
       "       [ 4.65009579e-02],\n",
       "       [ 3.32136657e-02],\n",
       "       [ 1.15457362e-01],\n",
       "       [ 1.07117315e-01],\n",
       "       [ 9.41777645e-02],\n",
       "       [ 1.57775642e-02],\n",
       "       [ 8.38450109e-02],\n",
       "       [ 6.60607603e-02],\n",
       "       [ 6.66155122e-01],\n",
       "       [ 8.83761644e-02],\n",
       "       [ 3.56891306e-01],\n",
       "       [ 9.12618599e-01],\n",
       "       [ 3.92867462e-01],\n",
       "       [ 5.33573532e-01],\n",
       "       [ 1.02744259e-01],\n",
       "       [ 9.69697099e-01],\n",
       "       [ 3.91905508e-01],\n",
       "       [ 4.55080650e-01],\n",
       "       [ 3.72063461e-01],\n",
       "       [ 8.87570008e-02],\n",
       "       [ 2.64361604e-01],\n",
       "       [ 3.47106045e-01],\n",
       "       [ 3.78240232e-01],\n",
       "       [ 2.55085076e-01],\n",
       "       [ 1.55047347e-02],\n",
       "       [ 8.32477938e-02],\n",
       "       [ 8.58218220e-02],\n",
       "       [ 9.92419424e-02],\n",
       "       [ 1.28822459e-02],\n",
       "       [ 2.64808835e-02],\n",
       "       [ 8.07132391e-01],\n",
       "       [ 9.00813813e-01],\n",
       "       [ 8.63382572e-03],\n",
       "       [ 2.93322651e-02],\n",
       "       [ 7.40266593e-01],\n",
       "       [ 8.36148176e-01],\n",
       "       [ 8.38053947e-02],\n",
       "       [ 5.78544786e-02],\n",
       "       [ 3.68936071e-01],\n",
       "       [ 2.16502572e-01],\n",
       "       [ 2.00640518e-02],\n",
       "       [ 2.63645987e-01],\n",
       "       [ 3.76717417e-02],\n",
       "       [ 9.33417872e-01]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.array(y_test).reshape(-1,1)\n",
    "y_test = scaler_Y.transform(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MACHINE LEARNING ALGORITHM IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING FOR RANDOM FOREST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\B.Tech project\\venv\\lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 50}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RFmodel = RandomForestRegressor()\n",
    "\n",
    "n_estimators = [50,100,150,200,250,300]\n",
    "\n",
    "param_grid = {'n_estimators' : n_estimators}\n",
    "\n",
    "rf_grid = GridSearchCV(estimator = RFmodel, param_grid = param_grid, cv=5, verbose= 2, n_jobs = -1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\B.Tech project\\venv\\lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26719.7043388]\n",
      " [52494.6323046]\n",
      " [30060.7876964]\n",
      " [31149.9280077]\n",
      " [26086.8652541]\n",
      " [68610.9623033]\n",
      " [26055.3118165]\n",
      " [65459.2579713]\n",
      " [42756.4057041]\n",
      " [25814.0296686]\n",
      " [27916.4645682]\n",
      " [34563.771953 ]\n",
      " [52625.6166406]\n",
      " [67601.4621102]\n",
      " [68260.2821095]\n",
      " [44013.0400802]\n",
      " [69513.9851594]\n",
      " [26453.9958022]\n",
      " [42946.371131 ]\n",
      " [34547.0276162]\n",
      " [34560.6061328]\n",
      " [43174.5802338]\n",
      " [28924.2690643]\n",
      " [68159.788751 ]\n",
      " [68568.9571096]\n",
      " [51849.192305 ]\n",
      " [29280.0332439]\n",
      " [63103.3130487]\n",
      " [26817.9556856]\n",
      " [42040.0560929]\n",
      " [27760.0576927]\n",
      " [26389.6636921]\n",
      " [29846.3267765]\n",
      " [30301.6771485]\n",
      " [29469.6153702]\n",
      " [26125.3009175]\n",
      " [29297.248419 ]\n",
      " [28356.5869537]\n",
      " [52670.0939459]\n",
      " [29407.8207609]\n",
      " [42513.9481252]\n",
      " [68786.1680477]\n",
      " [44169.4080496]\n",
      " [52361.8060921]\n",
      " [30584.5529081]\n",
      " [70380.6941404]\n",
      " [43930.3973066]\n",
      " [44869.5236345]\n",
      " [42747.3195704]\n",
      " [29409.7566205]\n",
      " [37609.8878882]\n",
      " [43657.370586 ]\n",
      " [42047.3741392]\n",
      " [37542.3472242]\n",
      " [26004.0592362]\n",
      " [29299.772853 ]\n",
      " [29317.3405279]\n",
      " [29737.8805655]\n",
      " [25955.2385342]\n",
      " [26429.5086934]\n",
      " [63480.0095723]\n",
      " [63791.385431 ]\n",
      " [25494.410295 ]\n",
      " [26441.0599623]\n",
      " [62037.8150031]\n",
      " [64528.2142986]\n",
      " [29254.378068 ]\n",
      " [28209.9317764]\n",
      " [43999.3082829]\n",
      " [36373.6101174]\n",
      " [26327.3240836]\n",
      " [37382.9251933]\n",
      " [26781.4915248]\n",
      " [69340.2196893]]\n"
     ]
    }
   ],
   "source": [
    "RFmodel = RandomForestRegressor(n_estimators = 100)\n",
    "\n",
    "RFmodel.fit(X_train, y_train)\n",
    "\n",
    "predictions = RFmodel.predict(X_test)\n",
    "\n",
    "predictions = scaler_Y.inverse_transform(predictions.reshape(-1,1))\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPORTING THE RANDOM FOREST MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('RF_model.pkl', 'wb') as files:\n",
    "    pickle.dump(RFmodel,files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING FOR GRADIENT BOOSTING REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\B.Tech project\\venv\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'n_estimators': 100}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "GBmodel = GradientBoostingRegressor()\n",
    "\n",
    "n_estimators = [50,100,150,200]\n",
    "\n",
    "learning_rate = [0.001,0.01,0.1]\n",
    "\n",
    "param_grid_GB = {'n_estimators' : n_estimators,\n",
    "                 'learning_rate' : learning_rate}\n",
    "\n",
    "rf_grid_GB = GridSearchCV(estimator = GBmodel, param_grid = param_grid_GB, cv=5, verbose= 2, n_jobs = -1)\n",
    "rf_grid_GB.fit(X_train, y_train)\n",
    "rf_grid_GB.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRADIENT BOOSTING REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27026.73381736]\n",
      " [51941.2538512 ]\n",
      " [29943.802114  ]\n",
      " [31782.28947903]\n",
      " [25961.02356549]\n",
      " [69001.83155277]\n",
      " [25936.32547052]\n",
      " [65450.56693223]\n",
      " [42793.23821892]\n",
      " [26206.62707203]\n",
      " [27661.81931126]\n",
      " [34630.91790145]\n",
      " [52459.07126322]\n",
      " [67069.88445365]\n",
      " [68273.22337157]\n",
      " [43953.59069575]\n",
      " [69531.48059023]\n",
      " [26653.25769347]\n",
      " [43206.58715547]\n",
      " [34628.79682667]\n",
      " [34630.91790145]\n",
      " [43246.04861111]\n",
      " [29073.9875684 ]\n",
      " [67387.03111624]\n",
      " [68601.42300542]\n",
      " [51924.08339468]\n",
      " [29118.11830603]\n",
      " [63846.11965393]\n",
      " [26732.7770228 ]\n",
      " [42324.83887131]\n",
      " [27661.81931126]\n",
      " [26665.46809194]\n",
      " [29882.60006771]\n",
      " [30294.35091753]\n",
      " [29325.08424121]\n",
      " [26178.51556471]\n",
      " [29156.62852181]\n",
      " [28552.83071926]\n",
      " [52342.21924293]\n",
      " [29361.37505751]\n",
      " [42423.66184527]\n",
      " [69509.7437371 ]\n",
      " [43953.59069575]\n",
      " [51800.43356987]\n",
      " [30531.77030797]\n",
      " [70178.21284763]\n",
      " [43934.54127694]\n",
      " [44041.86886248]\n",
      " [42860.48514789]\n",
      " [29361.37505751]\n",
      " [37721.31846692]\n",
      " [43458.46305064]\n",
      " [42231.69881108]\n",
      " [37535.7037933 ]\n",
      " [26055.87522044]\n",
      " [29235.56376535]\n",
      " [29235.56376535]\n",
      " [29634.68170107]\n",
      " [25997.06040806]\n",
      " [26137.42801153]\n",
      " [64289.41367836]\n",
      " [65486.98089627]\n",
      " [25712.08374614]\n",
      " [26493.97325669]\n",
      " [62381.77794437]\n",
      " [65155.2509129 ]\n",
      " [29046.30784932]\n",
      " [28794.97464476]\n",
      " [43953.59069575]\n",
      " [36265.88988405]\n",
      " [25970.85561278]\n",
      " [37499.10197448]\n",
      " [26732.7770228 ]\n",
      " [69402.98898811]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\B.Tech project\\venv\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    }
   ],
   "source": [
    "GBModel = GradientBoostingRegressor(n_estimators= 100, learning_rate= 0.1)\n",
    "\n",
    "GBModel.fit(X_train,y_train)\n",
    "\n",
    "GBpredictions = GBModel.predict(X_test)\n",
    "\n",
    "GBpredictions = scaler_Y.inverse_transform(GBpredictions.reshape(-1,1))\n",
    "print(GBpredictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPORTING THE GRADIENT BOOSTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GB_model.pkl', 'wb') as files:\n",
    "    pickle.dump(GBModel,files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEEP LEARNING MODEL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.56553777],\n",
       "        [0.41114268],\n",
       "        [0.082805  ]],\n",
       "\n",
       "       [[0.58527185],\n",
       "        [0.59979477],\n",
       "        [0.37670751]],\n",
       "\n",
       "       [[0.49793023],\n",
       "        [0.97028613],\n",
       "        [0.96330439]],\n",
       "\n",
       "       [[0.43605963],\n",
       "        [0.88796932],\n",
       "        [0.8468105 ]],\n",
       "\n",
       "       [[0.86141616],\n",
       "        [0.6096598 ],\n",
       "        [0.40333791]],\n",
       "\n",
       "       [[0.36515427],\n",
       "        [0.85308527],\n",
       "        [0.76734921]],\n",
       "\n",
       "       [[0.85660636],\n",
       "        [0.42510714],\n",
       "        [0.10197558]],\n",
       "\n",
       "       [[0.9270644 ],\n",
       "        [0.47888701],\n",
       "        [0.18620838]],\n",
       "\n",
       "       [[0.96072976],\n",
       "        [0.48458952],\n",
       "        [0.19849244]],\n",
       "\n",
       "       [[0.19212341],\n",
       "        [0.41135387],\n",
       "        [0.082091  ]],\n",
       "\n",
       "       [[0.61016496],\n",
       "        [0.96762095],\n",
       "        [0.9682262 ]],\n",
       "\n",
       "       [[0.54502679],\n",
       "        [0.86013574],\n",
       "        [0.79567563]],\n",
       "\n",
       "       [[0.4576614 ],\n",
       "        [0.97517595],\n",
       "        [0.95335515]],\n",
       "\n",
       "       [[0.5287328 ],\n",
       "        [0.59946913],\n",
       "        [0.37528986]],\n",
       "\n",
       "       [[0.83339567],\n",
       "        [0.87091063],\n",
       "        [0.78537967]],\n",
       "\n",
       "       [[0.66132359],\n",
       "        [1.        ],\n",
       "        [0.98737439]],\n",
       "\n",
       "       [[0.45085635],\n",
       "        [0.90396836],\n",
       "        [0.86542775]],\n",
       "\n",
       "       [[0.83727993],\n",
       "        [0.42509872],\n",
       "        [0.10331786]],\n",
       "\n",
       "       [[0.66599781],\n",
       "        [0.60322627],\n",
       "        [0.39604146]],\n",
       "\n",
       "       [[0.23441379],\n",
       "        [0.57115788],\n",
       "        [0.32990932]],\n",
       "\n",
       "       [[0.65649291],\n",
       "        [0.61046862],\n",
       "        [0.39099412]],\n",
       "\n",
       "       [[0.74225709],\n",
       "        [0.4102039 ],\n",
       "        [0.08254989]],\n",
       "\n",
       "       [[0.62032933],\n",
       "        [0.3815941 ],\n",
       "        [0.03612826]],\n",
       "\n",
       "       [[0.52105245],\n",
       "        [0.41144666],\n",
       "        [0.08166898]],\n",
       "\n",
       "       [[0.71790057],\n",
       "        [0.40187611],\n",
       "        [0.06735251]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.37582372],\n",
       "        [0.03049582]],\n",
       "\n",
       "       [[0.67744781],\n",
       "        [0.96124412],\n",
       "        [0.95126489]],\n",
       "\n",
       "       [[0.72221051],\n",
       "        [0.51666481],\n",
       "        [0.24184284]],\n",
       "\n",
       "       [[0.59746327],\n",
       "        [0.97828522],\n",
       "        [0.96509188]],\n",
       "\n",
       "       [[0.64262328],\n",
       "        [0.52626956],\n",
       "        [0.26411245]],\n",
       "\n",
       "       [[0.84636487],\n",
       "        [0.42424796],\n",
       "        [0.0952626 ]],\n",
       "\n",
       "       [[0.68590555],\n",
       "        [0.64351534],\n",
       "        [0.446611  ]],\n",
       "\n",
       "       [[0.94198204],\n",
       "        [0.48031679],\n",
       "        [0.19019418]],\n",
       "\n",
       "       [[0.42306529],\n",
       "        [0.89108769],\n",
       "        [0.85242585]],\n",
       "\n",
       "       [[0.53355158],\n",
       "        [0.57619498],\n",
       "        [0.34851652]],\n",
       "\n",
       "       [[0.54613147],\n",
       "        [0.64046649],\n",
       "        [0.42380368]],\n",
       "\n",
       "       [[0.71562428],\n",
       "        [0.37393189],\n",
       "        [0.02425726]],\n",
       "\n",
       "       [[0.55924164],\n",
       "        [0.95961791],\n",
       "        [0.9385    ]],\n",
       "\n",
       "       [[0.52010787],\n",
       "        [0.60849239],\n",
       "        [0.39267289]],\n",
       "\n",
       "       [[0.50425806],\n",
       "        [0.95682565],\n",
       "        [0.93773097]],\n",
       "\n",
       "       [[0.82547892],\n",
       "        [0.85389752],\n",
       "        [0.7655873 ]],\n",
       "\n",
       "       [[0.76585133],\n",
       "        [0.55340523],\n",
       "        [0.31515624]],\n",
       "\n",
       "       [[0.4724505 ],\n",
       "        [0.95697721],\n",
       "        [0.92344134]],\n",
       "\n",
       "       [[0.85653938],\n",
       "        [0.49019506],\n",
       "        [0.20385084]],\n",
       "\n",
       "       [[0.41780355],\n",
       "        [0.59103915],\n",
       "        [0.36280429]],\n",
       "\n",
       "       [[0.14471667],\n",
       "        [0.36480654],\n",
       "        [0.01026843]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.01850632]],\n",
       "\n",
       "       [[0.67250845],\n",
       "        [0.49307176],\n",
       "        [0.21208542]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.36042994],\n",
       "        [0.00518239]],\n",
       "\n",
       "       [[0.80812584],\n",
       "        [0.80294536],\n",
       "        [0.71812099]],\n",
       "\n",
       "       [[0.86737798],\n",
       "        [0.636727  ],\n",
       "        [0.44640798]],\n",
       "\n",
       "       [[0.87477169],\n",
       "        [0.50391277],\n",
       "        [0.23365035]],\n",
       "\n",
       "       [[0.48709058],\n",
       "        [0.38194149],\n",
       "        [0.03316846]],\n",
       "\n",
       "       [[0.60664712],\n",
       "        [0.97230837],\n",
       "        [0.9459356 ]],\n",
       "\n",
       "       [[0.68414381],\n",
       "        [0.39481772],\n",
       "        [0.0567622 ]],\n",
       "\n",
       "       [[0.77519023],\n",
       "        [0.90474275],\n",
       "        [0.85470512]],\n",
       "\n",
       "       [[0.97129124],\n",
       "        [0.72648655],\n",
       "        [0.56847525]],\n",
       "\n",
       "       [[0.51063311],\n",
       "        [0.42474436],\n",
       "        [0.10496457]],\n",
       "\n",
       "       [[0.60624997],\n",
       "        [0.9148379 ],\n",
       "        [0.87928713]],\n",
       "\n",
       "       [[0.56348148],\n",
       "        [0.42488249],\n",
       "        [0.10468294]],\n",
       "\n",
       "       [[0.52915298],\n",
       "        [0.98244693],\n",
       "        [0.96848141]],\n",
       "\n",
       "       [[0.39636108],\n",
       "        [0.36071656],\n",
       "        [0.0050444 ]],\n",
       "\n",
       "       [[0.49230582],\n",
       "        [0.37165614],\n",
       "        [0.0142566 ]],\n",
       "\n",
       "       [[0.66790425],\n",
       "        [0.62996394],\n",
       "        [0.43264389]],\n",
       "\n",
       "       [[0.40221469],\n",
       "        [0.95332944],\n",
       "        [0.91535351]],\n",
       "\n",
       "       [[0.62752013],\n",
       "        [0.51098635],\n",
       "        [0.2405594 ]],\n",
       "\n",
       "       [[0.53377982],\n",
       "        [0.95422576],\n",
       "        [0.93252738]],\n",
       "\n",
       "       [[0.69489033],\n",
       "        [0.60479831],\n",
       "        [0.37521572]],\n",
       "\n",
       "       [[0.54699935],\n",
       "        [0.51861524],\n",
       "        [0.25024472]],\n",
       "\n",
       "       [[0.57677661],\n",
       "        [0.37236223],\n",
       "        [0.01921063]],\n",
       "\n",
       "       [[0.38530522],\n",
       "        [0.41605194],\n",
       "        [0.08889539]],\n",
       "\n",
       "       [[0.57915158],\n",
       "        [0.95271715],\n",
       "        [0.94465216]],\n",
       "\n",
       "       [[0.8988068 ],\n",
       "        [0.58536592],\n",
       "        [0.36272232]],\n",
       "\n",
       "       [[0.21502767],\n",
       "        [0.37400214],\n",
       "        [0.02450399]],\n",
       "\n",
       "       [[0.89848946],\n",
       "        [0.42351309],\n",
       "        [0.10949406]],\n",
       "\n",
       "       [[0.69150083],\n",
       "        [0.95626918],\n",
       "        [0.94275956]],\n",
       "\n",
       "       [[0.59551903],\n",
       "        [0.51664618],\n",
       "        [0.24745281]],\n",
       "\n",
       "       [[0.57792896],\n",
       "        [0.98095385],\n",
       "        [0.94898303]],\n",
       "\n",
       "       [[0.47567184],\n",
       "        [0.42714535],\n",
       "        [0.10404743]],\n",
       "\n",
       "       [[0.73827258],\n",
       "        [0.72095095],\n",
       "        [0.56664589]],\n",
       "\n",
       "       [[0.88573163],\n",
       "        [0.65129876],\n",
       "        [0.46355493]],\n",
       "\n",
       "       [[0.55905597],\n",
       "        [0.98274023],\n",
       "        [0.97506205]],\n",
       "\n",
       "       [[0.45340226],\n",
       "        [0.91474018],\n",
       "        [0.87885971]],\n",
       "\n",
       "       [[0.58180158],\n",
       "        [0.42982863],\n",
       "        [0.11007544]],\n",
       "\n",
       "       [[0.21826941],\n",
       "        [0.56200205],\n",
       "        [0.32176147]],\n",
       "\n",
       "       [[0.67241787],\n",
       "        [0.52965325],\n",
       "        [0.27160828]],\n",
       "\n",
       "       [[0.83788886],\n",
       "        [0.50978898],\n",
       "        [0.23993191]],\n",
       "\n",
       "       [[0.53914926],\n",
       "        [0.52510182],\n",
       "        [0.25944528]],\n",
       "\n",
       "       [[0.93371493],\n",
       "        [0.43800786],\n",
       "        [0.13343383]],\n",
       "\n",
       "       [[0.64933245],\n",
       "        [0.58893698],\n",
       "        [0.36874189]],\n",
       "\n",
       "       [[0.82093783],\n",
       "        [0.94786412],\n",
       "        [0.92468786]],\n",
       "\n",
       "       [[0.86059781],\n",
       "        [0.62242556],\n",
       "        [0.42643485]],\n",
       "\n",
       "       [[0.95789626],\n",
       "        [0.70424485],\n",
       "        [0.54339473]],\n",
       "\n",
       "       [[0.64318815],\n",
       "        [0.4281931 ],\n",
       "        [0.11244503]],\n",
       "\n",
       "       [[0.21601314],\n",
       "        [0.56684633],\n",
       "        [0.32565079]],\n",
       "\n",
       "       [[0.65472502],\n",
       "        [0.6368595 ],\n",
       "        [0.44216425]],\n",
       "\n",
       "       [[0.48097344],\n",
       "        [0.5700688 ],\n",
       "        [0.34267082]],\n",
       "\n",
       "       [[0.50777753],\n",
       "        [0.59362201],\n",
       "        [0.36785666]],\n",
       "\n",
       "       [[0.45072072],\n",
       "        [0.41747585],\n",
       "        [0.09327909]],\n",
       "\n",
       "       [[0.6160327 ],\n",
       "        [0.95709768],\n",
       "        [0.93875899]],\n",
       "\n",
       "       [[0.39626915],\n",
       "        [0.37627404],\n",
       "        [0.0306516 ]],\n",
       "\n",
       "       [[0.50737208],\n",
       "        [0.36245793],\n",
       "        [0.00624055]],\n",
       "\n",
       "       [[0.54670245],\n",
       "        [0.36841593],\n",
       "        [0.01561321]],\n",
       "\n",
       "       [[0.53735584],\n",
       "        [0.51430861],\n",
       "        [0.24169551]],\n",
       "\n",
       "       [[0.9041899 ],\n",
       "        [0.67949788],\n",
       "        [0.50136044]],\n",
       "\n",
       "       [[0.39976296],\n",
       "        [0.59153668],\n",
       "        [0.36869013]],\n",
       "\n",
       "       [[0.82671253],\n",
       "        [0.42493003],\n",
       "        [0.09733145]],\n",
       "\n",
       "       [[0.89058377],\n",
       "        [0.7264243 ],\n",
       "        [0.57562691]],\n",
       "\n",
       "       [[0.14590676],\n",
       "        [0.36555468],\n",
       "        [0.01095034]],\n",
       "\n",
       "       [[0.38680988],\n",
       "        [0.91451806],\n",
       "        [0.86397607]],\n",
       "\n",
       "       [[0.18933347],\n",
       "        [0.39160549],\n",
       "        [0.04153252]],\n",
       "\n",
       "       [[0.19539648],\n",
       "        [0.57512026],\n",
       "        [0.33884311]],\n",
       "\n",
       "       [[0.54686819],\n",
       "        [0.97797935],\n",
       "        [0.97159145]],\n",
       "\n",
       "       [[0.72104814],\n",
       "        [0.96500988],\n",
       "        [0.9475836 ]],\n",
       "\n",
       "       [[0.54621657],\n",
       "        [0.59800468],\n",
       "        [0.38356879]],\n",
       "\n",
       "       [[0.68873771],\n",
       "        [0.51501757],\n",
       "        [0.24538245]],\n",
       "\n",
       "       [[0.3975849 ],\n",
       "        [0.95141296],\n",
       "        [0.93535924]],\n",
       "\n",
       "       [[0.59186594],\n",
       "        [0.51226705],\n",
       "        [0.2405448 ]],\n",
       "\n",
       "       [[0.39623849],\n",
       "        [0.37267984],\n",
       "        [0.0267016 ]],\n",
       "\n",
       "       [[0.73427461],\n",
       "        [0.5941118 ],\n",
       "        [0.37417272]],\n",
       "\n",
       "       [[0.66126105],\n",
       "        [0.97615331],\n",
       "        [0.97611367]],\n",
       "\n",
       "       [[0.60759918],\n",
       "        [0.38521969],\n",
       "        [0.04042243]],\n",
       "\n",
       "       [[0.48770773],\n",
       "        [0.85260122],\n",
       "        [0.80330967]],\n",
       "\n",
       "       [[0.51016764],\n",
       "        [0.59576399],\n",
       "        [0.36931727]],\n",
       "\n",
       "       [[0.71846692],\n",
       "        [0.8754501 ],\n",
       "        [0.8281658 ]],\n",
       "\n",
       "       [[0.59530401],\n",
       "        [0.3690505 ],\n",
       "        [0.01577457]],\n",
       "\n",
       "       [[0.86188795],\n",
       "        [0.48912768],\n",
       "        [0.20249735]],\n",
       "\n",
       "       [[0.80311356],\n",
       "        [0.51429529],\n",
       "        [0.24350724]],\n",
       "\n",
       "       [[0.70569946],\n",
       "        [0.49049   ],\n",
       "        [0.20818612]],\n",
       "\n",
       "       [[0.85673845],\n",
       "        [0.41822603],\n",
       "        [0.0898596 ]],\n",
       "\n",
       "       [[0.63722657],\n",
       "        [0.59843741],\n",
       "        [0.37390646]],\n",
       "\n",
       "       [[0.06854202],\n",
       "        [0.36502136],\n",
       "        [0.01591451]],\n",
       "\n",
       "       [[0.70279586],\n",
       "        [0.37003062],\n",
       "        [0.0175063 ]],\n",
       "\n",
       "       [[0.57514462],\n",
       "        [0.98204984],\n",
       "        [0.97825038]],\n",
       "\n",
       "       [[0.51472734],\n",
       "        [0.59539561],\n",
       "        [0.37115187]],\n",
       "\n",
       "       [[0.95874735],\n",
       "        [0.71505997],\n",
       "        [0.55655682]],\n",
       "\n",
       "       [[1.        ],\n",
       "        [0.45149043],\n",
       "        [0.1538018 ]],\n",
       "\n",
       "       [[0.56915507],\n",
       "        [0.94798156],\n",
       "        [0.92856417]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.36555664],\n",
       "        [0.00929251]],\n",
       "\n",
       "       [[0.30305781],\n",
       "        [0.35981396],\n",
       "        [0.00128926]],\n",
       "\n",
       "       [[0.77344871],\n",
       "        [0.39568849],\n",
       "        [0.06610297]],\n",
       "\n",
       "       [[0.63580777],\n",
       "        [0.40869543],\n",
       "        [0.08115957]],\n",
       "\n",
       "       [[0.55645481],\n",
       "        [0.41101853],\n",
       "        [0.0819446 ]],\n",
       "\n",
       "       [[0.43375983],\n",
       "        [0.88358253],\n",
       "        [0.83764195]],\n",
       "\n",
       "       [[0.42870291],\n",
       "        [0.58862949],\n",
       "        [0.36474768]],\n",
       "\n",
       "       [[0.80571414],\n",
       "        [0.497578  ],\n",
       "        [0.2222935 ]],\n",
       "\n",
       "       [[0.51504178],\n",
       "        [0.59595731],\n",
       "        [0.38143751]],\n",
       "\n",
       "       [[0.73355757],\n",
       "        [0.99892588],\n",
       "        [1.        ]],\n",
       "\n",
       "       [[0.68548794],\n",
       "        [0.96226976],\n",
       "        [0.94676473]],\n",
       "\n",
       "       [[0.49553621],\n",
       "        [0.86432377],\n",
       "        [0.7987862 ]],\n",
       "\n",
       "       [[0.42088604],\n",
       "        [0.42205895],\n",
       "        [0.09933625]],\n",
       "\n",
       "       [[0.611245  ],\n",
       "        [0.36965276],\n",
       "        [0.02040888]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.38617322],\n",
       "        [0.05236603]],\n",
       "\n",
       "       [[0.61845159],\n",
       "        [0.37503166],\n",
       "        [0.02222518]],\n",
       "\n",
       "       [[0.42570698],\n",
       "        [0.58867157],\n",
       "        [0.35922627]],\n",
       "\n",
       "       [[0.78482652],\n",
       "        [0.97743959],\n",
       "        [0.97401003]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.0112444 ]],\n",
       "\n",
       "       [[0.89251918],\n",
       "        [0.66707547],\n",
       "        [0.48718738]],\n",
       "\n",
       "       [[0.57568846],\n",
       "        [0.38533192],\n",
       "        [0.03766391]],\n",
       "\n",
       "       [[0.85830493],\n",
       "        [0.42580239],\n",
       "        [0.10641284]],\n",
       "\n",
       "       [[0.48694093],\n",
       "        [0.42472878],\n",
       "        [0.10334496]],\n",
       "\n",
       "       [[0.15326447],\n",
       "        [0.36627525],\n",
       "        [0.01455813]],\n",
       "\n",
       "       [[0.46255814],\n",
       "        [0.56276036],\n",
       "        [0.33297348]],\n",
       "\n",
       "       [[0.722791  ],\n",
       "        [0.59607552],\n",
       "        [0.37028628]],\n",
       "\n",
       "       [[0.8348668 ],\n",
       "        [0.61264068],\n",
       "        [0.38785625]],\n",
       "\n",
       "       [[0.44898639],\n",
       "        [0.86201017],\n",
       "        [0.78935277]],\n",
       "\n",
       "       [[0.82297903],\n",
       "        [0.93279496],\n",
       "        [0.89417828]],\n",
       "\n",
       "       [[0.58162756],\n",
       "        [0.9280567 ],\n",
       "        [0.88398653]],\n",
       "\n",
       "       [[0.83763356],\n",
       "        [0.4271004 ],\n",
       "        [0.10176798]],\n",
       "\n",
       "       [[0.39741726],\n",
       "        [0.92378459],\n",
       "        [0.87960213]],\n",
       "\n",
       "       [[0.63084287],\n",
       "        [0.9648267 ],\n",
       "        [0.94133405]],\n",
       "\n",
       "       [[0.86758831],\n",
       "        [0.5960413 ],\n",
       "        [0.37109988]],\n",
       "\n",
       "       [[0.50688836],\n",
       "        [0.90167889],\n",
       "        [0.8470222 ]],\n",
       "\n",
       "       [[0.1655251 ],\n",
       "        [0.36571703],\n",
       "        [0.01046881]],\n",
       "\n",
       "       [[0.32900401],\n",
       "        [0.40971664],\n",
       "        [0.08150223]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.013758  ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.36812812],\n",
       "        [0.01638267]],\n",
       "\n",
       "       [[0.70586859],\n",
       "        [0.37539575],\n",
       "        [0.02698138]],\n",
       "\n",
       "       [[0.74002983],\n",
       "        [0.89448741],\n",
       "        [0.83725163]],\n",
       "\n",
       "       [[0.49475144],\n",
       "        [0.3709922 ],\n",
       "        [0.01105358]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.36329901],\n",
       "        [0.01144901]],\n",
       "\n",
       "       [[0.50573826],\n",
       "        [0.4286486 ],\n",
       "        [0.1092478 ]],\n",
       "\n",
       "       [[0.69541891],\n",
       "        [0.96206086],\n",
       "        [0.94715085]],\n",
       "\n",
       "       [[0.39087349],\n",
       "        [0.40874702],\n",
       "        [0.07694154]],\n",
       "\n",
       "       [[0.45235797],\n",
       "        [0.42088369],\n",
       "        [0.0974285 ]],\n",
       "\n",
       "       [[0.31365126],\n",
       "        [0.40921312],\n",
       "        [0.08157203]],\n",
       "\n",
       "       [[0.47867983],\n",
       "        [0.6053208 ],\n",
       "        [0.39005405]],\n",
       "\n",
       "       [[0.42217818],\n",
       "        [0.8651016 ],\n",
       "        [0.78301128]],\n",
       "\n",
       "       [[0.49697593],\n",
       "        [0.92662151],\n",
       "        [0.89452351]],\n",
       "\n",
       "       [[0.59238627],\n",
       "        [0.9537982 ],\n",
       "        [0.92977447]],\n",
       "\n",
       "       [[0.67422949],\n",
       "        [0.38840382],\n",
       "        [0.04562555]],\n",
       "\n",
       "       [[0.45353651],\n",
       "        [0.59626645],\n",
       "        [0.38427704]],\n",
       "\n",
       "       [[0.56271204],\n",
       "        [0.41005883],\n",
       "        [0.08272702]],\n",
       "\n",
       "       [[0.81828061],\n",
       "        [0.93326622],\n",
       "        [0.91019274]],\n",
       "\n",
       "       [[0.63659592],\n",
       "        [0.38087879],\n",
       "        [0.04141585]],\n",
       "\n",
       "       [[0.68374976],\n",
       "        [0.59964447],\n",
       "        [0.36592171]],\n",
       "\n",
       "       [[0.83516586],\n",
       "        [0.42599091],\n",
       "        [0.10503985]],\n",
       "\n",
       "       [[0.46093138],\n",
       "        [0.41852227],\n",
       "        [0.09467499]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.01871043]],\n",
       "\n",
       "       [[0.57577102],\n",
       "        [0.60669578],\n",
       "        [0.38428623]],\n",
       "\n",
       "       [[0.50222356],\n",
       "        [0.98123679],\n",
       "        [0.97325067]],\n",
       "\n",
       "       [[0.61079234],\n",
       "        [0.36251801],\n",
       "        [0.00983036]],\n",
       "\n",
       "       [[0.69701976],\n",
       "        [0.54187591],\n",
       "        [0.29324067]],\n",
       "\n",
       "       [[0.9411472 ],\n",
       "        [0.72635354],\n",
       "        [0.57048359]],\n",
       "\n",
       "       [[0.49181519],\n",
       "        [0.36260407],\n",
       "        [0.00544143]],\n",
       "\n",
       "       [[0.47848225],\n",
       "        [0.87769343],\n",
       "        [0.81010413]],\n",
       "\n",
       "       [[0.43027755],\n",
       "        [0.36095205],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.41371238],\n",
       "        [0.90664337],\n",
       "        [0.87017937]],\n",
       "\n",
       "       [[0.53852979],\n",
       "        [0.5195488 ],\n",
       "        [0.25526321]],\n",
       "\n",
       "       [[0.41495892],\n",
       "        [0.40834662],\n",
       "        [0.07590656]],\n",
       "\n",
       "       [[0.40069057],\n",
       "        [0.41292346],\n",
       "        [0.08406321]],\n",
       "\n",
       "       [[0.60767708],\n",
       "        [0.59670827],\n",
       "        [0.38182196]],\n",
       "\n",
       "       [[0.74501716],\n",
       "        [0.99082015],\n",
       "        [0.99702412]],\n",
       "\n",
       "       [[0.42328462],\n",
       "        [0.58417029],\n",
       "        [0.35596266]],\n",
       "\n",
       "       [[0.85926471],\n",
       "        [0.48750573],\n",
       "        [0.20010598]],\n",
       "\n",
       "       [[0.94779495],\n",
       "        [0.47888829],\n",
       "        [0.18204872]],\n",
       "\n",
       "       [[0.95188695],\n",
       "        [0.57121809],\n",
       "        [0.34516593]],\n",
       "\n",
       "       [[0.63617321],\n",
       "        [0.38551935],\n",
       "        [0.04783165]],\n",
       "\n",
       "       [[0.42889102],\n",
       "        [0.8924645 ],\n",
       "        [0.83927269]],\n",
       "\n",
       "       [[0.55737899],\n",
       "        [0.98060376],\n",
       "        [0.97100159]],\n",
       "\n",
       "       [[0.72575736],\n",
       "        [0.71940786],\n",
       "        [0.56892747]],\n",
       "\n",
       "       [[0.49890351],\n",
       "        [0.60203344],\n",
       "        [0.38212915]],\n",
       "\n",
       "       [[0.44690515],\n",
       "        [0.37246447],\n",
       "        [0.01980972]],\n",
       "\n",
       "       [[0.68760403],\n",
       "        [0.3875943 ],\n",
       "        [0.045839  ]],\n",
       "\n",
       "       [[0.48225144],\n",
       "        [0.90630504],\n",
       "        [0.86611438]],\n",
       "\n",
       "       [[0.39849068],\n",
       "        [0.88480807],\n",
       "        [0.81523251]],\n",
       "\n",
       "       [[0.52807336],\n",
       "        [0.3624038 ],\n",
       "        [0.00738314]],\n",
       "\n",
       "       [[0.39368315],\n",
       "        [0.36918435],\n",
       "        [0.02202031]],\n",
       "\n",
       "       [[0.82144396],\n",
       "        [0.83289703],\n",
       "        [0.74840768]],\n",
       "\n",
       "       [[0.63690556],\n",
       "        [0.38647286],\n",
       "        [0.03983948]],\n",
       "\n",
       "       [[0.48807953],\n",
       "        [0.92457469],\n",
       "        [0.91261277]],\n",
       "\n",
       "       [[0.37743508],\n",
       "        [0.40927287],\n",
       "        [0.07819263]],\n",
       "\n",
       "       [[0.94183569],\n",
       "        [0.7273223 ],\n",
       "        [0.57553004]],\n",
       "\n",
       "       [[0.53853101],\n",
       "        [0.95757475],\n",
       "        [0.95281387]],\n",
       "\n",
       "       [[0.50479326],\n",
       "        [0.41947444],\n",
       "        [0.09526294]],\n",
       "\n",
       "       [[0.56282441],\n",
       "        [0.37539394],\n",
       "        [0.02465998]],\n",
       "\n",
       "       [[0.79672035],\n",
       "        [0.91843726],\n",
       "        [0.87234259]],\n",
       "\n",
       "       [[0.37158977],\n",
       "        [0.91595232],\n",
       "        [0.86902864]],\n",
       "\n",
       "       [[0.48352081],\n",
       "        [0.42727658],\n",
       "        [0.10488565]],\n",
       "\n",
       "       [[0.33199996],\n",
       "        [0.40950141],\n",
       "        [0.08009619]],\n",
       "\n",
       "       [[0.41375463],\n",
       "        [0.88888383],\n",
       "        [0.83654482]],\n",
       "\n",
       "       [[0.5062659 ],\n",
       "        [0.37692124],\n",
       "        [0.02945859]],\n",
       "\n",
       "       [[0.49384318],\n",
       "        [0.91885384],\n",
       "        [0.88175402]],\n",
       "\n",
       "       [[0.51908414],\n",
       "        [0.42792441],\n",
       "        [0.10455434]],\n",
       "\n",
       "       [[0.87566495],\n",
       "        [0.60775694],\n",
       "        [0.38309182]],\n",
       "\n",
       "       [[0.61002473],\n",
       "        [0.5112195 ],\n",
       "        [0.24048864]],\n",
       "\n",
       "       [[0.73464216],\n",
       "        [0.48907647],\n",
       "        [0.20466531]],\n",
       "\n",
       "       [[0.82067993],\n",
       "        [0.60319998],\n",
       "        [0.38898822]],\n",
       "\n",
       "       [[0.55010359],\n",
       "        [0.60514315],\n",
       "        [0.38657688]],\n",
       "\n",
       "       [[0.48073607],\n",
       "        [0.42303265],\n",
       "        [0.10198734]],\n",
       "\n",
       "       [[0.53721784],\n",
       "        [0.42275377],\n",
       "        [0.10286544]],\n",
       "\n",
       "       [[0.51338868],\n",
       "        [0.3789408 ],\n",
       "        [0.03094548]],\n",
       "\n",
       "       [[0.86393985],\n",
       "        [0.61179884],\n",
       "        [0.39142632]],\n",
       "\n",
       "       [[0.47554724],\n",
       "        [0.85953059],\n",
       "        [0.78862135]],\n",
       "\n",
       "       [[0.44937171],\n",
       "        [0.9137208 ],\n",
       "        [0.87581929]],\n",
       "\n",
       "       [[0.49222323],\n",
       "        [0.85246037],\n",
       "        [0.78451964]],\n",
       "\n",
       "       [[0.90055166],\n",
       "        [0.48696349],\n",
       "        [0.20036633]],\n",
       "\n",
       "       [[0.48424985],\n",
       "        [0.93029667],\n",
       "        [0.89094455]],\n",
       "\n",
       "       [[0.63241752],\n",
       "        [0.90353866],\n",
       "        [0.86495347]],\n",
       "\n",
       "       [[0.70998206],\n",
       "        [0.53586928],\n",
       "        [0.28237933]],\n",
       "\n",
       "       [[0.64301053],\n",
       "        [0.36469001],\n",
       "        [0.01352698]],\n",
       "\n",
       "       [[0.42223077],\n",
       "        [0.88004738],\n",
       "        [0.80396511]],\n",
       "\n",
       "       [[0.51101899],\n",
       "        [0.60537061],\n",
       "        [0.39016768]],\n",
       "\n",
       "       [[0.78603819],\n",
       "        [0.60003243],\n",
       "        [0.37827716]],\n",
       "\n",
       "       [[0.719449  ],\n",
       "        [0.37829871],\n",
       "        [0.03359238]],\n",
       "\n",
       "       [[0.87806033],\n",
       "        [0.77601508],\n",
       "        [0.68635151]],\n",
       "\n",
       "       [[0.19890192],\n",
       "        [0.3823402 ],\n",
       "        [0.03090466]],\n",
       "\n",
       "       [[0.40773832],\n",
       "        [0.87582491],\n",
       "        [0.81952187]],\n",
       "\n",
       "       [[0.8588764 ],\n",
       "        [0.42879603],\n",
       "        [0.11125521]],\n",
       "\n",
       "       [[0.65328859],\n",
       "        [0.51526237],\n",
       "        [0.2508579 ]],\n",
       "\n",
       "       [[0.44129518],\n",
       "        [0.86863481],\n",
       "        [0.79764921]],\n",
       "\n",
       "       [[0.70991221],\n",
       "        [0.97111426],\n",
       "        [0.94182947]],\n",
       "\n",
       "       [[0.51801251],\n",
       "        [0.52650467],\n",
       "        [0.26360145]],\n",
       "\n",
       "       [[0.68695458],\n",
       "        [0.96048264],\n",
       "        [0.94593713]],\n",
       "\n",
       "       [[0.41326518],\n",
       "        [0.42658912],\n",
       "        [0.10282865]],\n",
       "\n",
       "       [[0.54701511],\n",
       "        [0.5907371 ],\n",
       "        [0.36443408]],\n",
       "\n",
       "       [[0.42109858],\n",
       "        [0.8897474 ],\n",
       "        [0.83853698]],\n",
       "\n",
       "       [[0.48629019],\n",
       "        [0.3619215 ],\n",
       "        [0.00757382]],\n",
       "\n",
       "       [[0.21207715],\n",
       "        [0.40065012],\n",
       "        [0.05798728]],\n",
       "\n",
       "       [[0.49558974],\n",
       "        [0.36607505],\n",
       "        [0.00965697]],\n",
       "\n",
       "       [[0.49742583],\n",
       "        [0.93715255],\n",
       "        [0.93339804]],\n",
       "\n",
       "       [[0.78247034],\n",
       "        [0.60000754],\n",
       "        [0.37577546]],\n",
       "\n",
       "       [[0.82321831],\n",
       "        [0.96469569],\n",
       "        [0.95710781]],\n",
       "\n",
       "       [[0.52702581],\n",
       "        [0.61628764],\n",
       "        [0.39954696]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.36213118],\n",
       "        [0.00970976]],\n",
       "\n",
       "       [[0.54428589],\n",
       "        [0.94950501],\n",
       "        [0.92995299]],\n",
       "\n",
       "       [[0.51513317],\n",
       "        [0.5934478 ],\n",
       "        [0.37508627]],\n",
       "\n",
       "       [[0.96073306],\n",
       "        [0.69258778],\n",
       "        [0.52608898]],\n",
       "\n",
       "       [[0.42876767],\n",
       "        [0.91124153],\n",
       "        [0.85528248]],\n",
       "\n",
       "       [[0.57543651],\n",
       "        [0.59148454],\n",
       "        [0.36008174]],\n",
       "\n",
       "       [[0.55064335],\n",
       "        [0.38273661],\n",
       "        [0.03707983]],\n",
       "\n",
       "       [[0.36331769],\n",
       "        [0.41447252],\n",
       "        [0.08543489]],\n",
       "\n",
       "       [[0.55540605],\n",
       "        [0.52460342],\n",
       "        [0.25797979]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reshaping Data for LSTM and GRU models\n",
    "#data = data.reshape((data.shape[0], data.shape[1], 1))\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[6.16766309e-01],\n",
       "        [3.75520880e-01],\n",
       "        [2.31998801e-02]],\n",
       "\n",
       "       [[8.28898121e-01],\n",
       "        [7.25422806e-01],\n",
       "        [5.71775170e-01]],\n",
       "\n",
       "       [[5.08513408e-01],\n",
       "        [4.18113562e-01],\n",
       "        [9.46411860e-02]],\n",
       "\n",
       "       [[7.51214367e-01],\n",
       "        [4.29725374e-01],\n",
       "        [1.11706024e-01]],\n",
       "\n",
       "       [[1.50573586e-01],\n",
       "        [3.65878267e-01],\n",
       "        [1.25659081e-02]],\n",
       "\n",
       "       [[7.06027469e-01],\n",
       "        [9.42548625e-01],\n",
       "        [9.25019897e-01]],\n",
       "\n",
       "       [[4.40304909e-01],\n",
       "        [3.61364902e-01],\n",
       "        [4.66683388e-03]],\n",
       "\n",
       "       [[4.07811450e-01],\n",
       "        [9.43817851e-01],\n",
       "        [9.04074626e-01]],\n",
       "\n",
       "       [[6.24466008e-01],\n",
       "        [6.01655033e-01],\n",
       "        [3.77772976e-01]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [3.60115388e-01],\n",
       "        [1.50077500e-03]],\n",
       "\n",
       "       [[6.85745462e-01],\n",
       "        [3.86368934e-01],\n",
       "        [4.33413558e-02]],\n",
       "\n",
       "       [[9.44227002e-01],\n",
       "        [4.74299913e-01],\n",
       "        [1.70393530e-01]],\n",
       "\n",
       "       [[7.61756752e-01],\n",
       "        [7.26913607e-01],\n",
       "        [5.90896126e-01]],\n",
       "\n",
       "       [[5.49135251e-01],\n",
       "        [9.26128422e-01],\n",
       "        [9.03746695e-01]],\n",
       "\n",
       "       [[5.87811929e-01],\n",
       "        [9.67920249e-01],\n",
       "        [9.41741448e-01]],\n",
       "\n",
       "       [[5.33622243e-01],\n",
       "        [6.10979772e-01],\n",
       "        [3.95825590e-01]],\n",
       "\n",
       "       [[5.09407082e-01],\n",
       "        [9.71725162e-01],\n",
       "        [9.59440784e-01]],\n",
       "\n",
       "       [[6.08255064e-01],\n",
       "        [3.70582415e-01],\n",
       "        [1.68906257e-02]],\n",
       "\n",
       "       [[4.59554031e-01],\n",
       "        [6.05255930e-01],\n",
       "        [3.84002511e-01]],\n",
       "\n",
       "       [[9.72905100e-01],\n",
       "        [4.63345999e-01],\n",
       "        [1.64842635e-01]],\n",
       "\n",
       "       [[9.46232405e-01],\n",
       "        [4.77106544e-01],\n",
       "        [1.75412060e-01]],\n",
       "\n",
       "       [[4.83090370e-01],\n",
       "        [6.10578022e-01],\n",
       "        [3.89039532e-01]],\n",
       "\n",
       "       [[3.13956202e-01],\n",
       "        [4.06973886e-01],\n",
       "        [7.61054852e-02]],\n",
       "\n",
       "       [[4.64315707e-01],\n",
       "        [9.53031583e-01],\n",
       "        [9.43351147e-01]],\n",
       "\n",
       "       [[6.22570172e-01],\n",
       "        [9.44018862e-01],\n",
       "        [9.29075917e-01]],\n",
       "\n",
       "       [[9.32077757e-01],\n",
       "        [7.19828710e-01],\n",
       "        [5.61686475e-01]],\n",
       "\n",
       "       [[4.04971451e-01],\n",
       "        [4.10293349e-01],\n",
       "        [8.03828454e-02]],\n",
       "\n",
       "       [[4.93774611e-01],\n",
       "        [8.62095989e-01],\n",
       "        [8.16860223e-01]],\n",
       "\n",
       "       [[5.76312948e-01],\n",
       "        [3.71484694e-01],\n",
       "        [2.28008504e-02]],\n",
       "\n",
       "       [[6.34439399e-01],\n",
       "        [5.91723873e-01],\n",
       "        [3.67396613e-01]],\n",
       "\n",
       "       [[6.57245955e-01],\n",
       "        [3.88334187e-01],\n",
       "        [4.31257409e-02]],\n",
       "\n",
       "       [[6.64836167e-01],\n",
       "        [3.72053669e-01],\n",
       "        [1.93011454e-02]],\n",
       "\n",
       "       [[8.03178284e-01],\n",
       "        [4.07882589e-01],\n",
       "        [8.10025526e-02]],\n",
       "\n",
       "       [[4.90909483e-01],\n",
       "        [4.28316661e-01],\n",
       "        [1.06835811e-01]],\n",
       "\n",
       "       [[4.84559221e-01],\n",
       "        [4.10802264e-01],\n",
       "        [8.28555884e-02]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [3.63907670e-01],\n",
       "        [8.11023090e-03]],\n",
       "\n",
       "       [[5.20439434e-01],\n",
       "        [4.07053942e-01],\n",
       "        [7.65651035e-02]],\n",
       "\n",
       "       [[5.86373399e-01],\n",
       "        [3.89620077e-01],\n",
       "        [5.14680626e-02]],\n",
       "\n",
       "       [[8.33853345e-01],\n",
       "        [7.43083049e-01],\n",
       "        [6.24331233e-01]],\n",
       "\n",
       "       [[5.36482627e-01],\n",
       "        [4.12138123e-01],\n",
       "        [8.26232942e-02]],\n",
       "\n",
       "       [[5.74824870e-01],\n",
       "        [5.94681699e-01],\n",
       "        [3.69187057e-01]],\n",
       "\n",
       "       [[6.37778161e-01],\n",
       "        [9.41764138e-01],\n",
       "        [9.36369936e-01]],\n",
       "\n",
       "       [[5.31029873e-01],\n",
       "        [6.15789338e-01],\n",
       "        [4.00755966e-01]],\n",
       "\n",
       "       [[7.30929425e-01],\n",
       "        [7.21532903e-01],\n",
       "        [5.65006462e-01]],\n",
       "\n",
       "       [[8.58829099e-01],\n",
       "        [4.25396226e-01],\n",
       "        [1.00312963e-01]],\n",
       "\n",
       "       [[5.67544208e-01],\n",
       "        [9.68523360e-01],\n",
       "        [9.66806447e-01]],\n",
       "\n",
       "       [[5.65309418e-01],\n",
       "        [6.12950451e-01],\n",
       "        [4.01224021e-01]],\n",
       "\n",
       "       [[6.54365001e-01],\n",
       "        [6.24481791e-01],\n",
       "        [4.23730226e-01]],\n",
       "\n",
       "       [[6.06680571e-01],\n",
       "        [6.00827047e-01],\n",
       "        [3.78931284e-01]],\n",
       "\n",
       "       [[5.32364831e-01],\n",
       "        [4.12796567e-01],\n",
       "        [8.26359492e-02]],\n",
       "\n",
       "       [[6.33264448e-01],\n",
       "        [5.26118052e-01],\n",
       "        [2.61232237e-01]],\n",
       "\n",
       "       [[5.03756098e-01],\n",
       "        [6.16769034e-01],\n",
       "        [3.90661289e-01]],\n",
       "\n",
       "       [[5.62619347e-01],\n",
       "        [5.85177551e-01],\n",
       "        [3.61596951e-01]],\n",
       "\n",
       "       [[6.48741309e-01],\n",
       "        [5.16391422e-01],\n",
       "        [2.46312513e-01]],\n",
       "\n",
       "       [[5.04340329e-01],\n",
       "        [3.62225024e-01],\n",
       "        [6.74146909e-03]],\n",
       "\n",
       "       [[3.94201716e-01],\n",
       "        [4.10334314e-01],\n",
       "        [8.08553630e-02]],\n",
       "\n",
       "       [[3.24576530e-01],\n",
       "        [4.09850678e-01],\n",
       "        [8.08574114e-02]],\n",
       "\n",
       "       [[7.32285210e-01],\n",
       "        [4.05789360e-01],\n",
       "        [7.61389060e-02]],\n",
       "\n",
       "       [[4.92339704e-01],\n",
       "        [3.61497740e-01],\n",
       "        [5.94727435e-03]],\n",
       "\n",
       "       [[1.57945632e-01],\n",
       "        [3.66090156e-01],\n",
       "        [1.62850973e-02]],\n",
       "\n",
       "       [[5.35408971e-01],\n",
       "        [8.91245504e-01],\n",
       "        [8.39442574e-01]],\n",
       "\n",
       "       [[8.91725103e-01],\n",
       "        [8.87206580e-01],\n",
       "        [8.36425277e-01]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [3.59346565e-01],\n",
       "        [4.06058968e-04]],\n",
       "\n",
       "       [[6.27294818e-01],\n",
       "        [3.68621953e-01],\n",
       "        [1.56951347e-02]],\n",
       "\n",
       "       [[4.18232316e-01],\n",
       "        [8.80490345e-01],\n",
       "        [8.16065093e-01]],\n",
       "\n",
       "       [[6.69942120e-01],\n",
       "        [8.87864106e-01],\n",
       "        [8.42231568e-01]],\n",
       "\n",
       "       [[4.15497175e-01],\n",
       "        [4.09835730e-01],\n",
       "        [7.97177711e-02]],\n",
       "\n",
       "       [[7.37221948e-01],\n",
       "        [3.89007241e-01],\n",
       "        [4.72566653e-02]],\n",
       "\n",
       "       [[5.43789339e-01],\n",
       "        [6.28918946e-01],\n",
       "        [4.08519731e-01]],\n",
       "\n",
       "       [[5.95715970e-01],\n",
       "        [5.13432590e-01],\n",
       "        [2.33660827e-01]],\n",
       "\n",
       "       [[1.88253165e-01],\n",
       "        [3.66791745e-01],\n",
       "        [1.97571113e-02]],\n",
       "\n",
       "       [[5.50683124e-01],\n",
       "        [5.20443469e-01],\n",
       "        [2.59169775e-01]],\n",
       "\n",
       "       [[5.94025048e-01],\n",
       "        [3.73357298e-01],\n",
       "        [2.48074695e-02]],\n",
       "\n",
       "       [[5.27198211e-01],\n",
       "        [9.79162190e-01],\n",
       "        [9.63920209e-01]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LONG SHORT-TERM MEMORY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "#Creating a LSTM model\n",
    "\n",
    "def create_model(units = 50):\n",
    "    regressor = Sequential()\n",
    "\n",
    "    regressor.add(LSTM(units=units, return_sequences = True, input_shape = (X_train.shape[1],X_train.shape[2])))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    regressor.add(LSTM(units= units,return_sequences= True))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    regressor.add(LSTM(units= units))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    regressor.add(Dense(units=1))\n",
    "    regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING FOR LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\B.Tech project\\venv\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\B.Tech project\\venv\\lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\nihar\\B.Tech project\\venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - loss: 0.2221\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0905\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0904\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0844\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0760\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0722\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0585\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0499\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0386\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0306\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0178\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0160\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0077\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0073\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0071\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0058\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0049\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0054\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0058\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0086\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0061\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0049\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0044\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0049\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0065\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0064\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0035\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0044\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0040\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0068\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0035\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0028\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0038\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0037\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0034\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0034\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0028\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0035\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0029\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0029\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0019\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0026\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0025\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0020\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0019\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0024\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0023\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0022\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0024\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32, 'epochs': 100, 'units': 100}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "LSTM_model = KerasRegressor(build_fn= create_model,units=50)\n",
    "batch_size = [16,32]\n",
    "epochs = [50,100]\n",
    "units = [50,100]\n",
    "\n",
    "param_grid_LSTM = {'batch_size' : batch_size,\n",
    "                 'epochs' : epochs,\n",
    "                 'units': units}\n",
    "\n",
    "rf_grid_LSTM = GridSearchCV(estimator = LSTM_model, param_grid = param_grid_LSTM, cv=5, verbose= 2, n_jobs = -1)\n",
    "rf_grid_LSTM.fit(X_train, y_train)\n",
    "\n",
    "rf_grid_LSTM.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2285\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0952\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0889\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0906\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0694\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0604\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0472\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0418\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0286\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0190\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0114\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0080\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0084\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0071\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0057\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0054\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0065\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0051\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0055\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0064\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0059\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0055\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0057\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0042\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0058\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0061\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0056\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0058\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0053\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0048\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 \n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0044\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0043\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0033\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0032\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0029\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0034\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0028\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0038\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0031\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0036\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0031\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0037\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0041\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0034\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0034\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0028\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0026\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0024\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0032\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0025\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0025\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0025\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0037\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0028\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0031 \n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 \n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0027\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0027\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024  \n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0035\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020    \n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019  \n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0025\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0023\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0019\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0026\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0017\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0020\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0024\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0021\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0021\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000290540ABEB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000290540ABEB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 784ms/stepWARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000290540ABEB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000290540ABEB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 374ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27352.92 ]\n",
      " [52848.613]\n",
      " [30402.99 ]\n",
      " [31371.062]\n",
      " [26789.627]\n",
      " [68369.03 ]\n",
      " [26462.752]\n",
      " [66039.266]\n",
      " [43294.637]\n",
      " [26400.174]\n",
      " [28243.127]\n",
      " [34406.277]\n",
      " [53271.035]\n",
      " [66465.92 ]\n",
      " [68742.32 ]\n",
      " [43834.438]\n",
      " [68909.72 ]\n",
      " [27054.764]\n",
      " [43152.75 ]\n",
      " [34032.77 ]\n",
      " [34636.324]\n",
      " [43474.574]\n",
      " [29496.96 ]\n",
      " [67764.555]\n",
      " [68049.914]\n",
      " [52757.125]\n",
      " [29736.746]\n",
      " [62137.664]\n",
      " [27258.158]\n",
      " [42786.05 ]\n",
      " [28258.016]\n",
      " [27176.676]\n",
      " [29967.006]\n",
      " [30967.291]\n",
      " [29873.549]\n",
      " [26661.354]\n",
      " [29624.084]\n",
      " [28526.988]\n",
      " [54989.594]\n",
      " [29921.986]\n",
      " [42755.824]\n",
      " [68320.43 ]\n",
      " [44079.49 ]\n",
      " [52188.766]\n",
      " [30987.658]\n",
      " [69419.766]\n",
      " [44131.816]\n",
      " [45336.543]\n",
      " [43272.79 ]\n",
      " [29931.074]\n",
      " [38017.348]\n",
      " [43695.32 ]\n",
      " [42298.074]\n",
      " [37365.863]\n",
      " [26553.879]\n",
      " [29746.822]\n",
      " [29702.459]\n",
      " [29719.787]\n",
      " [26513.71 ]\n",
      " [26909.418]\n",
      " [63653.71 ]\n",
      " [65325.277]\n",
      " [26354.838]\n",
      " [26986.07 ]\n",
      " [62095.402]\n",
      " [64380.27 ]\n",
      " [29713.3  ]\n",
      " [28444.77 ]\n",
      " [44614.785]\n",
      " [36801.426]\n",
      " [27027.389]\n",
      " [37699.21 ]\n",
      " [27362.238]\n",
      " [69307.64 ]]\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters and re-create the model\n",
    "best_params = rf_grid_LSTM.best_params_\n",
    "best_units = best_params['units']\n",
    "best_batch_size = best_params['batch_size']\n",
    "best_epochs = best_params['epochs']\n",
    "\n",
    "# Re-create the best model\n",
    "best_model = create_model(best_units)\n",
    "\n",
    "# Fit the best model on the training data\n",
    "best_model.fit(X_train, y_train, batch_size=best_batch_size, epochs=best_epochs)\n",
    "\n",
    "# Make predictions\n",
    "LSTMpredictions = best_model.predict(X_test)\n",
    "\n",
    "# Assuming `scaler` is the same MinMaxScaler used for scaling y_train and y_test\n",
    "LSTMpredictions = scaler_Y.inverse_transform(LSTMpredictions.reshape(-1, 1))\n",
    "print(LSTMpredictions)\n",
    "\n",
    "# Save the best model properly using Keras' save function\n",
    "best_model.save('LSTM_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPORTING THE LSTM MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRATED RECURRENT UNIT MODEL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "#Creating a GRU model\n",
    "\n",
    "def create_model_GRU(units):\n",
    "    regressor = Sequential()\n",
    "\n",
    "    regressor.add(GRU(units=units, return_sequences = True, input_shape = (X_train.shape[1],X_train.shape[2])))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    regressor.add(GRU(units= units,return_sequences= True))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    regressor.add(GRU(units= units))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    regressor.add(Dense(units=1))\n",
    "    regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING FOR GRU MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\B.Tech project\\venv\\lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\nihar\\B.Tech project\\venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 0.1555\n",
      "Epoch 2/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0699\n",
      "Epoch 3/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0507\n",
      "Epoch 4/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0309\n",
      "Epoch 5/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0146\n",
      "Epoch 6/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0104\n",
      "Epoch 7/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0098\n",
      "Epoch 8/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0083\n",
      "Epoch 9/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - loss: 0.0057\n",
      "Epoch 10/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0063\n",
      "Epoch 11/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046\n",
      "Epoch 12/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0053\n",
      "Epoch 13/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0058\n",
      "Epoch 14/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0062\n",
      "Epoch 15/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0061\n",
      "Epoch 16/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0067\n",
      "Epoch 17/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0055\n",
      "Epoch 18/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0060\n",
      "Epoch 19/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041\n",
      "Epoch 20/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0056\n",
      "Epoch 21/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044\n",
      "Epoch 22/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0052\n",
      "Epoch 23/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0036\n",
      "Epoch 24/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0043\n",
      "Epoch 25/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047\n",
      "Epoch 26/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0038\n",
      "Epoch 27/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0048\n",
      "Epoch 28/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0034\n",
      "Epoch 29/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053\n",
      "Epoch 30/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039\n",
      "Epoch 31/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038\n",
      "Epoch 32/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0041\n",
      "Epoch 33/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0028\n",
      "Epoch 34/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0042\n",
      "Epoch 35/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0033\n",
      "Epoch 36/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0030\n",
      "Epoch 37/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0033\n",
      "Epoch 38/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041\n",
      "Epoch 39/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0036\n",
      "Epoch 40/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0028\n",
      "Epoch 41/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0033\n",
      "Epoch 42/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028\n",
      "Epoch 43/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0028\n",
      "Epoch 44/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037\n",
      "Epoch 45/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0046\n",
      "Epoch 46/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026\n",
      "Epoch 47/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032\n",
      "Epoch 48/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026\n",
      "Epoch 49/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0029\n",
      "Epoch 50/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028\n",
      "Epoch 51/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0031\n",
      "Epoch 52/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026\n",
      "Epoch 53/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023\n",
      "Epoch 54/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0033\n",
      "Epoch 55/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028\n",
      "Epoch 56/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030\n",
      "Epoch 57/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0030 \n",
      "Epoch 58/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029\n",
      "Epoch 59/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023\n",
      "Epoch 60/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0037\n",
      "Epoch 61/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026\n",
      "Epoch 62/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027\n",
      "Epoch 63/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0034\n",
      "Epoch 64/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044\n",
      "Epoch 65/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0027\n",
      "Epoch 66/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026\n",
      "Epoch 67/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0029\n",
      "Epoch 68/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0028\n",
      "Epoch 69/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026\n",
      "Epoch 70/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027\n",
      "Epoch 71/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031\n",
      "Epoch 72/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0021\n",
      "Epoch 73/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0028\n",
      "Epoch 74/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0027\n",
      "Epoch 75/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020\n",
      "Epoch 76/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0028\n",
      "Epoch 77/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0021\n",
      "Epoch 78/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024\n",
      "Epoch 79/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023\n",
      "Epoch 80/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023\n",
      "Epoch 81/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0031\n",
      "Epoch 82/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0025\n",
      "Epoch 83/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022\n",
      "Epoch 84/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0036\n",
      "Epoch 85/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0040\n",
      "Epoch 86/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0028\n",
      "Epoch 87/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0024\n",
      "Epoch 88/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0022\n",
      "Epoch 89/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024\n",
      "Epoch 90/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0018\n",
      "Epoch 91/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026\n",
      "Epoch 92/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024\n",
      "Epoch 93/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0022\n",
      "Epoch 94/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023\n",
      "Epoch 95/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020\n",
      "Epoch 96/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026\n",
      "Epoch 97/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023\n",
      "Epoch 98/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0032\n",
      "Epoch 99/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0025\n",
      "Epoch 100/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0025\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model_GRU at 0x00000290540ABB50&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=100\n",
       "\tunits=50\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;KerasRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function create_model_GRU at 0x00000290540ABB50&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=100\n",
       "\tunits=50\n",
       ")</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=None\n",
       "\tbuild_fn=<function create_model_GRU at 0x00000290540ABB50>\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=100\n",
       "\tunits=50\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRU_model = KerasRegressor(build_fn= create_model_GRU, units=50)\n",
    "batch_size = [16,32]\n",
    "epochs = [50,100]       #32, 100, 100 best estimator\n",
    "units = [50,100]\n",
    "\n",
    "param_grid_GRU = {'batch_size' : batch_size,\n",
    "                 'epochs' : epochs,\n",
    "                 'units': units}\n",
    "\n",
    "rf_grid_GRU = GridSearchCV(estimator = GRU_model, param_grid = param_grid_GRU, cv=5, verbose= 2, n_jobs = -1)\n",
    "rf_grid_GRU.fit(X_train, y_train)\n",
    "\n",
    "best_model = rf_grid_GRU.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\B.Tech project\\venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - loss: 0.1312\n",
      "Epoch 2/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0674\n",
      "Epoch 3/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0464\n",
      "Epoch 4/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0240\n",
      "Epoch 5/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0177\n",
      "Epoch 6/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0120\n",
      "Epoch 7/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0094\n",
      "Epoch 8/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0066\n",
      "Epoch 9/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0069\n",
      "Epoch 10/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0047\n",
      "Epoch 11/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0056\n",
      "Epoch 12/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0060\n",
      "Epoch 13/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0067\n",
      "Epoch 14/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0064\n",
      "Epoch 15/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0037\n",
      "Epoch 16/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0054\n",
      "Epoch 17/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0060\n",
      "Epoch 18/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051\n",
      "Epoch 19/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0054\n",
      "Epoch 20/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0044\n",
      "Epoch 21/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0053\n",
      "Epoch 22/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0039\n",
      "Epoch 23/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0040\n",
      "Epoch 24/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0033\n",
      "Epoch 25/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0051\n",
      "Epoch 26/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0051\n",
      "Epoch 27/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050\n",
      "Epoch 28/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041\n",
      "Epoch 29/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0037\n",
      "Epoch 30/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0049\n",
      "Epoch 31/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0031\n",
      "Epoch 32/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0031\n",
      "Epoch 33/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0037\n",
      "Epoch 34/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0029\n",
      "Epoch 35/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0039\n",
      "Epoch 36/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0029\n",
      "Epoch 37/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0032\n",
      "Epoch 38/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0032\n",
      "Epoch 39/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0033\n",
      "Epoch 40/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0032\n",
      "Epoch 41/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0029\n",
      "Epoch 42/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0035\n",
      "Epoch 43/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0036\n",
      "Epoch 44/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0037\n",
      "Epoch 45/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0037\n",
      "Epoch 46/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0029\n",
      "Epoch 47/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0032\n",
      "Epoch 48/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0024   \n",
      "Epoch 49/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0034\n",
      "Epoch 50/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0036\n",
      "Epoch 51/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0035\n",
      "Epoch 52/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044\n",
      "Epoch 53/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023\n",
      "Epoch 54/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025\n",
      "Epoch 55/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0024\n",
      "Epoch 56/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0029\n",
      "Epoch 57/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0023\n",
      "Epoch 58/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0025\n",
      "Epoch 59/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036\n",
      "Epoch 60/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0029\n",
      "Epoch 61/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023\n",
      "Epoch 62/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0032\n",
      "Epoch 63/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0024\n",
      "Epoch 64/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0028\n",
      "Epoch 65/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0025\n",
      "Epoch 66/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0024\n",
      "Epoch 67/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 68/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028   \n",
      "Epoch 69/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029\n",
      "Epoch 70/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0032\n",
      "Epoch 71/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0027\n",
      "Epoch 72/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0030\n",
      "Epoch 73/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0028\n",
      "Epoch 74/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0024\n",
      "Epoch 75/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0032\n",
      "Epoch 76/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0022\n",
      "Epoch 77/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0031\n",
      "Epoch 78/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0031\n",
      "Epoch 79/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0030\n",
      "Epoch 80/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0039\n",
      "Epoch 81/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0020\n",
      "Epoch 82/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0023\n",
      "Epoch 83/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0023\n",
      "Epoch 84/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0030\n",
      "Epoch 85/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0025\n",
      "Epoch 86/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0022\n",
      "Epoch 87/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037\n",
      "Epoch 88/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0033\n",
      "Epoch 89/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0026\n",
      "Epoch 90/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0039\n",
      "Epoch 91/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0031\n",
      "Epoch 92/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0028\n",
      "Epoch 93/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0025\n",
      "Epoch 94/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0023\n",
      "Epoch 95/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0025\n",
      "Epoch 96/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037\n",
      "Epoch 97/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018\n",
      "Epoch 98/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021\n",
      "Epoch 99/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0027\n",
      "Epoch 100/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0020   \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26974.701]\n",
      " [53341.15 ]\n",
      " [30063.777]\n",
      " [31340.914]\n",
      " [25997.965]\n",
      " [68706.96 ]\n",
      " [25818.615]\n",
      " [66207.21 ]\n",
      " [43557.88 ]\n",
      " [25720.535]\n",
      " [28026.143]\n",
      " [34397.055]\n",
      " [53925.734]\n",
      " [66906.98 ]\n",
      " [68815.19 ]\n",
      " [44086.676]\n",
      " [69079.04 ]\n",
      " [26665.766]\n",
      " [43302.668]\n",
      " [34069.082]\n",
      " [34636.145]\n",
      " [43613.508]\n",
      " [28897.45 ]\n",
      " [68122.86 ]\n",
      " [68408.64 ]\n",
      " [53164.29 ]\n",
      " [29217.45 ]\n",
      " [62841.617]\n",
      " [26877.158]\n",
      " [43092.79 ]\n",
      " [27972.928]\n",
      " [26875.082]\n",
      " [29952.982]\n",
      " [30592.238]\n",
      " [29473.809]\n",
      " [26011.98 ]\n",
      " [29254.193]\n",
      " [28220.041]\n",
      " [55692.145]\n",
      " [29567.326]\n",
      " [42989.652]\n",
      " [68772.41 ]\n",
      " [44309.93 ]\n",
      " [52675.336]\n",
      " [30956.389]\n",
      " [69696.78 ]\n",
      " [44439.664]\n",
      " [45778.383]\n",
      " [43547.887]\n",
      " [29560.312]\n",
      " [38122.594]\n",
      " [43766.19 ]\n",
      " [42584.176]\n",
      " [37463.188]\n",
      " [26015.082]\n",
      " [29221.734]\n",
      " [29125.113]\n",
      " [29631.271]\n",
      " [25958.805]\n",
      " [26165.562]\n",
      " [64085.766]\n",
      " [65707.92 ]\n",
      " [25672.795]\n",
      " [26641.291]\n",
      " [62488.5  ]\n",
      " [64896.36 ]\n",
      " [29204.594]\n",
      " [28290.969]\n",
      " [44732.656]\n",
      " [36743.438]\n",
      " [26313.527]\n",
      " [37787.434]\n",
      " [27002.293]\n",
      " [69385.65 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the best parameters and re-create the model\n",
    "best_params = rf_grid_GRU.best_params_\n",
    "best_units = best_params['units']\n",
    "best_batch_size = best_params['batch_size']\n",
    "best_epochs = best_params['epochs']\n",
    "\n",
    "# Re-create the best model\n",
    "best_model = create_model_GRU(best_units)\n",
    "\n",
    "# Fit the best model on the training data\n",
    "best_model.fit(X_train, y_train, batch_size=best_batch_size, epochs=best_epochs)\n",
    "\n",
    "# Make predictions\n",
    "GRUpredictions = best_model.predict(X_test)\n",
    "\n",
    "# Assuming `scaler` is the same MinMaxScaler used for scaling y_train and y_test\n",
    "GRUpredictions = scaler_Y.inverse_transform(GRUpredictions.reshape(-1, 1))\n",
    "print(GRUpredictions)\n",
    "\n",
    "# Save the best model properly using Keras' save function\n",
    "best_model.save('GRU_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPORTING THE GRU MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>RSI</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08-06-2023</td>\n",
       "      <td>26508.21680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26508.216800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09-06-2023</td>\n",
       "      <td>26480.37500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26498.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-06-2023</td>\n",
       "      <td>25851.24023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26283.037543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-06-2023</td>\n",
       "      <td>25940.16797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26168.747686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12-06-2023</td>\n",
       "      <td>25902.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26136.500000</td>\n",
       "      <td>26079.998457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>04-06-2024</td>\n",
       "      <td>70567.76563</td>\n",
       "      <td>51.839588</td>\n",
       "      <td>68464.500000</td>\n",
       "      <td>68980.494101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>05-06-2024</td>\n",
       "      <td>71082.82031</td>\n",
       "      <td>58.735150</td>\n",
       "      <td>69182.781250</td>\n",
       "      <td>69681.269504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>06-06-2024</td>\n",
       "      <td>70757.16406</td>\n",
       "      <td>63.653717</td>\n",
       "      <td>69792.826562</td>\n",
       "      <td>70039.901023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>07-06-2024</td>\n",
       "      <td>69342.58594</td>\n",
       "      <td>53.653915</td>\n",
       "      <td>70111.023438</td>\n",
       "      <td>69807.462662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>08-06-2024</td>\n",
       "      <td>69463.69531</td>\n",
       "      <td>50.936849</td>\n",
       "      <td>70242.806250</td>\n",
       "      <td>69692.873545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Close        RSI           SMA           EMA\n",
       "0    08-06-2023  26508.21680   0.000000      0.000000  26508.216800\n",
       "1    09-06-2023  26480.37500   0.000000      0.000000  26498.936200\n",
       "2    10-06-2023  25851.24023   0.000000      0.000000  26283.037543\n",
       "3    11-06-2023  25940.16797   0.000000      0.000000  26168.747686\n",
       "4    12-06-2023  25902.50000   0.000000  26136.500000  26079.998457\n",
       "..          ...          ...        ...           ...           ...\n",
       "362  04-06-2024  70567.76563  51.839588  68464.500000  68980.494101\n",
       "363  05-06-2024  71082.82031  58.735150  69182.781250  69681.269504\n",
       "364  06-06-2024  70757.16406  63.653717  69792.826562  70039.901023\n",
       "365  07-06-2024  69342.58594  53.653915  70111.023438  69807.462662\n",
       "366  08-06-2024  69463.69531  50.936849  70242.806250  69692.873545\n",
       "\n",
       "[367 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = df\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scaler_X.pkl', 'wb') as files:\n",
    "    pickle.dump(scaler_X,files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scaler_Y.pkl', 'wb') as files:\n",
    "    pickle.dump(scaler_Y,files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
